{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jhoffbauer/reddit_qa/venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2023-06-24 23:49:25.657147: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-06-24 23:49:26.292254: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please run\n",
      "\n",
      "python -m bitsandbytes\n",
      "\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "/home/jhoffbauer/reddit_qa/venv/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cpu.so: undefined symbol: cadam32bit_grad_fp32\n",
      "CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching in backup paths...\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 8.6\n",
      "CUDA SETUP: Detected CUDA version 117\n",
      "CUDA SETUP: Loading binary /home/jhoffbauer/reddit_qa/venv/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cpu.so...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jhoffbauer/reddit_qa/venv/lib/python3.10/site-packages/bitsandbytes/cextension.py:34: UserWarning: The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n",
      "  warn(\"The installed version of bitsandbytes was compiled without GPU support. \"\n",
      "/home/jhoffbauer/reddit_qa/venv/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:147: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('vs/workbench/api/node/extensionHostProcess')}\n",
      "  warn(msg)\n",
      "/home/jhoffbauer/reddit_qa/venv/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:147: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('module'), PosixPath('//matplotlib_inline.backend_inline')}\n",
      "  warn(msg)\n",
      "/home/jhoffbauer/reddit_qa/venv/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:147: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/usr/local/cuda/lib64')}\n",
      "  warn(msg)\n",
      "/home/jhoffbauer/reddit_qa/venv/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:147: UserWarning: WARNING: No libcudart.so found! Install CUDA or the cudatoolkit package (anaconda)!\n",
      "  warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "from uuid import uuid4\n",
    "import pickle as pkl\n",
    "from itertools import product\n",
    "from tqdm import tqdm\n",
    "\n",
    "from lm_eval import evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks = [\n",
    "    \"arc_easy\",\n",
    "    \"arc_challenge\",\n",
    "    \"boolq\",\n",
    "    \"hellaswag\",\n",
    "    \"openbookqa\",\n",
    "    \"winogrande\",\n",
    "    \"truthfulqa_gen\",\n",
    "    \"truthfulqa_mc\",\n",
    "    \"triviaqa\",\n",
    "]\n",
    "\n",
    "models = [\n",
    "    #\"gpt2\",\n",
    "    #\"EleutherAI/pythia-160M-Deduped\",\n",
    "    #\"EleutherAI/pythia-410M-Deduped\",\n",
    "    #\"EleutherAI/pythia-1B-Deduped\",\n",
    "    #\"EleutherAI/pythia-1.4B-Deduped\",\n",
    "    #\"EleutherAI/pythia-2.8B-Deduped\",\n",
    "    #\"EleutherAI/pythia-6.9B-Deduped\",\n",
    "    \"EleutherAI/pythia-12B-Deduped\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards:   0%|          | 0/3 [00:04<?, ?it/s]\n",
      "  0%|          | 0/1 [00:07<?, ?it/s]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "/home/jhoffbauer/reddit_qa/venv/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cpu.so: undefined symbol: cget_col_row_stats",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mfor\u001b[39;00m model \u001b[39min\u001b[39;00m tqdm(models):\n\u001b[0;32m----> 2\u001b[0m     results \u001b[39m=\u001b[39m evaluator\u001b[39m.\u001b[39;49msimple_evaluate(\n\u001b[1;32m      3\u001b[0m         model\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mhf-causal-experimental\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m      4\u001b[0m         model_args\u001b[39m=\u001b[39;49m\u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mpretrained=\u001b[39;49m\u001b[39m{\u001b[39;49;00mmodel\u001b[39m}\u001b[39;49;00m\u001b[39m,load_in_8bit=True,device_map_option=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mauto\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m,use_accelerate=True\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m      5\u001b[0m         tasks\u001b[39m=\u001b[39;49mtasks,\n\u001b[1;32m      6\u001b[0m         batch_size\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mauto\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m      7\u001b[0m         device\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mcuda\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m      8\u001b[0m     )\n\u001b[1;32m     11\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39meval_results/results-\u001b[39m\u001b[39m{\u001b[39;00mmodel\u001b[39m.\u001b[39mreplace(\u001b[39m\"\u001b[39m\u001b[39m/\u001b[39m\u001b[39m\"\u001b[39m,\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m-\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m}\u001b[39;00m\u001b[39m-\u001b[39m\u001b[39m{\u001b[39;00muuid4()\u001b[39m.\u001b[39mhex\u001b[39m}\u001b[39;00m\u001b[39m.pkl\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mwb\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mas\u001b[39;00m f:\n\u001b[1;32m     12\u001b[0m         pkl\u001b[39m.\u001b[39mdump(results, f)\n",
      "File \u001b[0;32m~/lm-evaluation-harness/lm_eval/utils.py:243\u001b[0m, in \u001b[0;36mpositional_deprecated.<locals>._wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    237\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(args) \u001b[39m!=\u001b[39m \u001b[39m1\u001b[39m \u001b[39mif\u001b[39;00m inspect\u001b[39m.\u001b[39mismethod(fn) \u001b[39melse\u001b[39;00m \u001b[39m0\u001b[39m:\n\u001b[1;32m    238\u001b[0m     \u001b[39mprint\u001b[39m(\n\u001b[1;32m    239\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mWARNING: using \u001b[39m\u001b[39m{\u001b[39;00mfn\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m with positional arguments is \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    240\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mdeprecated and will be disallowed in a future version of \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    241\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mlm-evaluation-harness!\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    242\u001b[0m     )\n\u001b[0;32m--> 243\u001b[0m \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/lm-evaluation-harness/lm_eval/evaluator.py:72\u001b[0m, in \u001b[0;36msimple_evaluate\u001b[0;34m(model, model_args, tasks, num_fewshot, batch_size, max_batch_size, device, no_cache, limit, bootstrap_iters, description_dict, check_integrity, decontamination_ngrams_path, write_out, output_base_path)\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[39mif\u001b[39;00m model_args \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     71\u001b[0m         model_args \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m---> 72\u001b[0m     lm \u001b[39m=\u001b[39m lm_eval\u001b[39m.\u001b[39;49mmodels\u001b[39m.\u001b[39;49mget_model(model)\u001b[39m.\u001b[39;49mcreate_from_arg_string(\n\u001b[1;32m     73\u001b[0m         model_args, {\u001b[39m\"\u001b[39;49m\u001b[39mbatch_size\u001b[39;49m\u001b[39m\"\u001b[39;49m: batch_size, \u001b[39m\"\u001b[39;49m\u001b[39mmax_batch_size\u001b[39;49m\u001b[39m\"\u001b[39;49m: max_batch_size, \u001b[39m\"\u001b[39;49m\u001b[39mdevice\u001b[39;49m\u001b[39m\"\u001b[39;49m: device}\n\u001b[1;32m     74\u001b[0m     )\n\u001b[1;32m     75\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     76\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39misinstance\u001b[39m(model, lm_eval\u001b[39m.\u001b[39mbase\u001b[39m.\u001b[39mLM)\n",
      "File \u001b[0;32m~/lm-evaluation-harness/lm_eval/base.py:115\u001b[0m, in \u001b[0;36mLM.create_from_arg_string\u001b[0;34m(cls, arg_string, additional_config)\u001b[0m\n\u001b[1;32m    113\u001b[0m args \u001b[39m=\u001b[39m utils\u001b[39m.\u001b[39msimple_parse_args_string(arg_string)\n\u001b[1;32m    114\u001b[0m args2 \u001b[39m=\u001b[39m {k: v \u001b[39mfor\u001b[39;00m k, v \u001b[39min\u001b[39;00m additional_config\u001b[39m.\u001b[39mitems() \u001b[39mif\u001b[39;00m v \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m}\n\u001b[0;32m--> 115\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mcls\u001b[39;49m(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49margs2)\n",
      "File \u001b[0;32m~/lm-evaluation-harness/lm_eval/models/huggingface.py:217\u001b[0m, in \u001b[0;36mHuggingFaceAutoLM.__init__\u001b[0;34m(self, pretrained, quantized, tokenizer, subfolder, revision, batch_size, max_batch_size, max_gen_toks, max_length, add_special_tokens, use_accelerate, device_map_option, max_memory_per_gpu, max_cpu_memory, offload_folder, dtype, device, peft, load_in_8bit, load_in_4bit, trust_remote_code, gptq_use_triton, bnb_4bit_quant_type, bnb_4bit_compute_dtype)\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[39mif\u001b[39;00m use_accelerate:\n\u001b[1;32m    211\u001b[0m     model_kwargs \u001b[39m=\u001b[39m _get_accelerate_args(\n\u001b[1;32m    212\u001b[0m         device_map_option,\n\u001b[1;32m    213\u001b[0m         max_memory_per_gpu,\n\u001b[1;32m    214\u001b[0m         max_cpu_memory,\n\u001b[1;32m    215\u001b[0m         offload_folder,\n\u001b[1;32m    216\u001b[0m     )\n\u001b[0;32m--> 217\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_create_auto_model(\n\u001b[1;32m    218\u001b[0m     pretrained\u001b[39m=\u001b[39;49mpretrained,\n\u001b[1;32m    219\u001b[0m     quantized\u001b[39m=\u001b[39;49mquantized,\n\u001b[1;32m    220\u001b[0m     trust_remote_code\u001b[39m=\u001b[39;49mtrust_remote_code,\n\u001b[1;32m    221\u001b[0m     revision\u001b[39m=\u001b[39;49mrevision,\n\u001b[1;32m    222\u001b[0m     subfolder\u001b[39m=\u001b[39;49msubfolder,\n\u001b[1;32m    223\u001b[0m     torch_dtype\u001b[39m=\u001b[39;49m_get_dtype(dtype, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_config),\n\u001b[1;32m    224\u001b[0m     gptq_use_triton\u001b[39m=\u001b[39;49mgptq_use_triton,\n\u001b[1;32m    225\u001b[0m     load_in_8bit\u001b[39m=\u001b[39;49mload_in_8bit,\n\u001b[1;32m    226\u001b[0m     load_in_4bit\u001b[39m=\u001b[39;49mload_in_4bit,\n\u001b[1;32m    227\u001b[0m     bnb_4bit_quant_type\u001b[39m=\u001b[39;49mbnb_4bit_quant_type,\n\u001b[1;32m    228\u001b[0m     bnb_4bit_compute_dtype\u001b[39m=\u001b[39;49mbnb_4bit_compute_dtype,\n\u001b[1;32m    229\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmodel_kwargs,\n\u001b[1;32m    230\u001b[0m )\n\u001b[1;32m    231\u001b[0m \u001b[39m# note: peft_path can be different than pretrained model path\u001b[39;00m\n\u001b[1;32m    232\u001b[0m \u001b[39mif\u001b[39;00m peft \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/lm-evaluation-harness/lm_eval/models/huggingface.py:283\u001b[0m, in \u001b[0;36mHuggingFaceAutoLM._create_auto_model\u001b[0;34m(self, pretrained, quantized, revision, subfolder, device_map, max_memory, offload_folder, load_in_8bit, load_in_4bit, trust_remote_code, torch_dtype, gptq_use_triton, bnb_4bit_quant_type, bnb_4bit_compute_dtype)\u001b[0m\n\u001b[1;32m    281\u001b[0m             model_kwargs[\u001b[39m\"\u001b[39m\u001b[39mbnb_4bit_quant_type\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m bnb_4bit_quant_type\n\u001b[1;32m    282\u001b[0m             model_kwargs[\u001b[39m\"\u001b[39m\u001b[39mbnb_4bit_compute_dtype\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(torch, bnb_4bit_compute_dtype)\n\u001b[0;32m--> 283\u001b[0m     model \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mAUTO_MODEL_CLASS\u001b[39m.\u001b[39;49mfrom_pretrained(\n\u001b[1;32m    284\u001b[0m         pretrained,\n\u001b[1;32m    285\u001b[0m         revision\u001b[39m=\u001b[39;49mrevision \u001b[39m+\u001b[39;49m (\u001b[39m\"\u001b[39;49m\u001b[39m/\u001b[39;49m\u001b[39m\"\u001b[39;49m \u001b[39m+\u001b[39;49m subfolder \u001b[39mif\u001b[39;49;00m subfolder \u001b[39mis\u001b[39;49;00m \u001b[39mnot\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m \u001b[39melse\u001b[39;49;00m \u001b[39m\"\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[1;32m    286\u001b[0m         device_map\u001b[39m=\u001b[39;49mdevice_map,\n\u001b[1;32m    287\u001b[0m         max_memory\u001b[39m=\u001b[39;49mmax_memory,\n\u001b[1;32m    288\u001b[0m         offload_folder\u001b[39m=\u001b[39;49moffload_folder,\n\u001b[1;32m    289\u001b[0m         load_in_8bit\u001b[39m=\u001b[39;49mload_in_8bit,\n\u001b[1;32m    290\u001b[0m         trust_remote_code\u001b[39m=\u001b[39;49mtrust_remote_code,\n\u001b[1;32m    291\u001b[0m         torch_dtype\u001b[39m=\u001b[39;49mtorch_dtype,\n\u001b[1;32m    292\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmodel_kwargs,\n\u001b[1;32m    293\u001b[0m     )\n\u001b[1;32m    294\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    295\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39mauto_gptq\u001b[39;00m \u001b[39mimport\u001b[39;00m AutoGPTQForCausalLM\n",
      "File \u001b[0;32m~/reddit_qa/venv/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:490\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    488\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mtype\u001b[39m(config) \u001b[39min\u001b[39;00m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_model_mapping\u001b[39m.\u001b[39mkeys():\n\u001b[1;32m    489\u001b[0m     model_class \u001b[39m=\u001b[39m _get_model_class(config, \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_model_mapping)\n\u001b[0;32m--> 490\u001b[0m     \u001b[39mreturn\u001b[39;00m model_class\u001b[39m.\u001b[39;49mfrom_pretrained(\n\u001b[1;32m    491\u001b[0m         pretrained_model_name_or_path, \u001b[39m*\u001b[39;49mmodel_args, config\u001b[39m=\u001b[39;49mconfig, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mhub_kwargs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs\n\u001b[1;32m    492\u001b[0m     )\n\u001b[1;32m    493\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    494\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mUnrecognized configuration class \u001b[39m\u001b[39m{\u001b[39;00mconfig\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m for this kind of AutoModel: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    495\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mModel type should be one of \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(c\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m \u001b[39m\u001b[39mfor\u001b[39;00m\u001b[39m \u001b[39mc\u001b[39m \u001b[39m\u001b[39min\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_model_mapping\u001b[39m.\u001b[39mkeys())\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    496\u001b[0m )\n",
      "File \u001b[0;32m~/reddit_qa/venv/lib/python3.10/site-packages/transformers/modeling_utils.py:2829\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   2819\u001b[0m     \u001b[39mif\u001b[39;00m dtype_orig \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   2820\u001b[0m         torch\u001b[39m.\u001b[39mset_default_dtype(dtype_orig)\n\u001b[1;32m   2822\u001b[0m     (\n\u001b[1;32m   2823\u001b[0m         model,\n\u001b[1;32m   2824\u001b[0m         missing_keys,\n\u001b[1;32m   2825\u001b[0m         unexpected_keys,\n\u001b[1;32m   2826\u001b[0m         mismatched_keys,\n\u001b[1;32m   2827\u001b[0m         offload_index,\n\u001b[1;32m   2828\u001b[0m         error_msgs,\n\u001b[0;32m-> 2829\u001b[0m     ) \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39;49m\u001b[39m.\u001b[39;49m_load_pretrained_model(\n\u001b[1;32m   2830\u001b[0m         model,\n\u001b[1;32m   2831\u001b[0m         state_dict,\n\u001b[1;32m   2832\u001b[0m         loaded_state_dict_keys,  \u001b[39m# XXX: rename?\u001b[39;49;00m\n\u001b[1;32m   2833\u001b[0m         resolved_archive_file,\n\u001b[1;32m   2834\u001b[0m         pretrained_model_name_or_path,\n\u001b[1;32m   2835\u001b[0m         ignore_mismatched_sizes\u001b[39m=\u001b[39;49mignore_mismatched_sizes,\n\u001b[1;32m   2836\u001b[0m         sharded_metadata\u001b[39m=\u001b[39;49msharded_metadata,\n\u001b[1;32m   2837\u001b[0m         _fast_init\u001b[39m=\u001b[39;49m_fast_init,\n\u001b[1;32m   2838\u001b[0m         low_cpu_mem_usage\u001b[39m=\u001b[39;49mlow_cpu_mem_usage,\n\u001b[1;32m   2839\u001b[0m         device_map\u001b[39m=\u001b[39;49mdevice_map,\n\u001b[1;32m   2840\u001b[0m         offload_folder\u001b[39m=\u001b[39;49moffload_folder,\n\u001b[1;32m   2841\u001b[0m         offload_state_dict\u001b[39m=\u001b[39;49moffload_state_dict,\n\u001b[1;32m   2842\u001b[0m         dtype\u001b[39m=\u001b[39;49mtorch_dtype,\n\u001b[1;32m   2843\u001b[0m         is_quantized\u001b[39m=\u001b[39;49m(load_in_8bit \u001b[39mor\u001b[39;49;00m load_in_4bit),\n\u001b[1;32m   2844\u001b[0m         keep_in_fp32_modules\u001b[39m=\u001b[39;49mkeep_in_fp32_modules,\n\u001b[1;32m   2845\u001b[0m     )\n\u001b[1;32m   2847\u001b[0m model\u001b[39m.\u001b[39mis_loaded_in_4bit \u001b[39m=\u001b[39m load_in_4bit\n\u001b[1;32m   2848\u001b[0m model\u001b[39m.\u001b[39mis_loaded_in_8bit \u001b[39m=\u001b[39m load_in_8bit\n",
      "File \u001b[0;32m~/reddit_qa/venv/lib/python3.10/site-packages/transformers/modeling_utils.py:3172\u001b[0m, in \u001b[0;36mPreTrainedModel._load_pretrained_model\u001b[0;34m(cls, model, state_dict, loaded_keys, resolved_archive_file, pretrained_model_name_or_path, ignore_mismatched_sizes, sharded_metadata, _fast_init, low_cpu_mem_usage, device_map, offload_folder, offload_state_dict, dtype, is_quantized, keep_in_fp32_modules)\u001b[0m\n\u001b[1;32m   3162\u001b[0m mismatched_keys \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m _find_mismatched_keys(\n\u001b[1;32m   3163\u001b[0m     state_dict,\n\u001b[1;32m   3164\u001b[0m     model_state_dict,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3168\u001b[0m     ignore_mismatched_sizes,\n\u001b[1;32m   3169\u001b[0m )\n\u001b[1;32m   3171\u001b[0m \u001b[39mif\u001b[39;00m low_cpu_mem_usage:\n\u001b[0;32m-> 3172\u001b[0m     new_error_msgs, offload_index, state_dict_index \u001b[39m=\u001b[39m _load_state_dict_into_meta_model(\n\u001b[1;32m   3173\u001b[0m         model_to_load,\n\u001b[1;32m   3174\u001b[0m         state_dict,\n\u001b[1;32m   3175\u001b[0m         loaded_keys,\n\u001b[1;32m   3176\u001b[0m         start_prefix,\n\u001b[1;32m   3177\u001b[0m         expected_keys,\n\u001b[1;32m   3178\u001b[0m         device_map\u001b[39m=\u001b[39;49mdevice_map,\n\u001b[1;32m   3179\u001b[0m         offload_folder\u001b[39m=\u001b[39;49moffload_folder,\n\u001b[1;32m   3180\u001b[0m         offload_index\u001b[39m=\u001b[39;49moffload_index,\n\u001b[1;32m   3181\u001b[0m         state_dict_folder\u001b[39m=\u001b[39;49mstate_dict_folder,\n\u001b[1;32m   3182\u001b[0m         state_dict_index\u001b[39m=\u001b[39;49mstate_dict_index,\n\u001b[1;32m   3183\u001b[0m         dtype\u001b[39m=\u001b[39;49mdtype,\n\u001b[1;32m   3184\u001b[0m         is_quantized\u001b[39m=\u001b[39;49mis_quantized,\n\u001b[1;32m   3185\u001b[0m         is_safetensors\u001b[39m=\u001b[39;49mis_safetensors,\n\u001b[1;32m   3186\u001b[0m         keep_in_fp32_modules\u001b[39m=\u001b[39;49mkeep_in_fp32_modules,\n\u001b[1;32m   3187\u001b[0m     )\n\u001b[1;32m   3188\u001b[0m     error_msgs \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m new_error_msgs\n\u001b[1;32m   3189\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/reddit_qa/venv/lib/python3.10/site-packages/transformers/modeling_utils.py:718\u001b[0m, in \u001b[0;36m_load_state_dict_into_meta_model\u001b[0;34m(model, state_dict, loaded_state_dict_keys, start_prefix, expected_keys, device_map, offload_folder, offload_index, state_dict_folder, state_dict_index, dtype, is_quantized, is_safetensors, keep_in_fp32_modules)\u001b[0m\n\u001b[1;32m    715\u001b[0m             fp16_statistics \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    717\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mSCB\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m param_name:\n\u001b[0;32m--> 718\u001b[0m             set_module_quantized_tensor_to_device(\n\u001b[1;32m    719\u001b[0m                 model, param_name, param_device, value\u001b[39m=\u001b[39;49mparam, fp16_statistics\u001b[39m=\u001b[39;49mfp16_statistics\n\u001b[1;32m    720\u001b[0m             )\n\u001b[1;32m    722\u001b[0m \u001b[39mreturn\u001b[39;00m error_msgs, offload_index, state_dict_index\n",
      "File \u001b[0;32m~/reddit_qa/venv/lib/python3.10/site-packages/transformers/utils/bitsandbytes.py:86\u001b[0m, in \u001b[0;36mset_module_quantized_tensor_to_device\u001b[0;34m(module, tensor_name, device, value, fp16_statistics)\u001b[0m\n\u001b[1;32m     84\u001b[0m kwargs \u001b[39m=\u001b[39m old_value\u001b[39m.\u001b[39m\u001b[39m__dict__\u001b[39m\n\u001b[1;32m     85\u001b[0m \u001b[39mif\u001b[39;00m is_8bit:\n\u001b[0;32m---> 86\u001b[0m     new_value \u001b[39m=\u001b[39m bnb\u001b[39m.\u001b[39;49mnn\u001b[39m.\u001b[39;49mInt8Params(new_value, requires_grad\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\u001b[39m.\u001b[39;49mto(device)\n\u001b[1;32m     87\u001b[0m \u001b[39melif\u001b[39;00m is_4bit:\n\u001b[1;32m     88\u001b[0m     new_value \u001b[39m=\u001b[39m bnb\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mParams4bit(new_value, requires_grad\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\u001b[39m.\u001b[39mto(device)\n",
      "File \u001b[0;32m~/reddit_qa/venv/lib/python3.10/site-packages/bitsandbytes/nn/modules.py:294\u001b[0m, in \u001b[0;36mInt8Params.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    285\u001b[0m device, dtype, non_blocking, convert_to_format \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39m_C\u001b[39m.\u001b[39m_nn\u001b[39m.\u001b[39m_parse_to(\n\u001b[1;32m    286\u001b[0m     \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs\n\u001b[1;32m    287\u001b[0m )\n\u001b[1;32m    289\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m    290\u001b[0m     device \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    291\u001b[0m     \u001b[39mand\u001b[39;00m device\u001b[39m.\u001b[39mtype \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    292\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mdevice\u001b[39m.\u001b[39mtype \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    293\u001b[0m ):\n\u001b[0;32m--> 294\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcuda(device)\n\u001b[1;32m    295\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    296\u001b[0m     new_param \u001b[39m=\u001b[39m Int8Params(\n\u001b[1;32m    297\u001b[0m         \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39mto(\n\u001b[1;32m    298\u001b[0m             device\u001b[39m=\u001b[39mdevice, dtype\u001b[39m=\u001b[39mdtype, non_blocking\u001b[39m=\u001b[39mnon_blocking\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    301\u001b[0m         has_fp16_weights\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhas_fp16_weights,\n\u001b[1;32m    302\u001b[0m     )\n",
      "File \u001b[0;32m~/reddit_qa/venv/lib/python3.10/site-packages/bitsandbytes/nn/modules.py:258\u001b[0m, in \u001b[0;36mInt8Params.cuda\u001b[0;34m(self, device)\u001b[0m\n\u001b[1;32m    254\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    255\u001b[0m     \u001b[39m# we store the 8-bit rows-major weight\u001b[39;00m\n\u001b[1;32m    256\u001b[0m     \u001b[39m# we convert this weight to the turning/ampere weight during the first inference pass\u001b[39;00m\n\u001b[1;32m    257\u001b[0m     B \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mcontiguous()\u001b[39m.\u001b[39mhalf()\u001b[39m.\u001b[39mcuda(device)\n\u001b[0;32m--> 258\u001b[0m     CB, CBt, SCB, SCBt, coo_tensorB \u001b[39m=\u001b[39m bnb\u001b[39m.\u001b[39;49mfunctional\u001b[39m.\u001b[39;49mdouble_quant(B)\n\u001b[1;32m    259\u001b[0m     \u001b[39mdel\u001b[39;00m CBt\n\u001b[1;32m    260\u001b[0m     \u001b[39mdel\u001b[39;00m SCBt\n",
      "File \u001b[0;32m~/reddit_qa/venv/lib/python3.10/site-packages/bitsandbytes/functional.py:2058\u001b[0m, in \u001b[0;36mdouble_quant\u001b[0;34m(A, col_stats, row_stats, out_col, out_row, threshold)\u001b[0m\n\u001b[1;32m   2055\u001b[0m     rows \u001b[39m=\u001b[39m A\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]\n\u001b[1;32m   2057\u001b[0m \u001b[39mif\u001b[39;00m row_stats \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m col_stats \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m-> 2058\u001b[0m     row_stats, col_stats, nnz_row_ptr \u001b[39m=\u001b[39m get_colrow_absmax(\n\u001b[1;32m   2059\u001b[0m         A, threshold\u001b[39m=\u001b[39;49mthreshold\n\u001b[1;32m   2060\u001b[0m     )\n\u001b[1;32m   2062\u001b[0m \u001b[39mif\u001b[39;00m out_col \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   2063\u001b[0m     out_col \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mzeros(A\u001b[39m.\u001b[39mshape, device\u001b[39m=\u001b[39mdevice, dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mint8)\n",
      "File \u001b[0;32m~/reddit_qa/venv/lib/python3.10/site-packages/bitsandbytes/functional.py:1947\u001b[0m, in \u001b[0;36mget_colrow_absmax\u001b[0;34m(A, row_stats, col_stats, nnz_block_ptr, threshold)\u001b[0m\n\u001b[1;32m   1945\u001b[0m prev_device \u001b[39m=\u001b[39m pre_call(A\u001b[39m.\u001b[39mdevice)\n\u001b[1;32m   1946\u001b[0m is_on_gpu([A, row_stats, col_stats, nnz_block_ptr])\n\u001b[0;32m-> 1947\u001b[0m lib\u001b[39m.\u001b[39;49mcget_col_row_stats(ptrA, ptrRowStats, ptrColStats, ptrNnzrows, ct\u001b[39m.\u001b[39mc_float(threshold), rows, cols)\n\u001b[1;32m   1948\u001b[0m post_call(prev_device)\n\u001b[1;32m   1950\u001b[0m \u001b[39mif\u001b[39;00m threshold \u001b[39m>\u001b[39m \u001b[39m0.0\u001b[39m:\n",
      "File \u001b[0;32m/usr/lib/python3.10/ctypes/__init__.py:387\u001b[0m, in \u001b[0;36mCDLL.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    385\u001b[0m \u001b[39mif\u001b[39;00m name\u001b[39m.\u001b[39mstartswith(\u001b[39m'\u001b[39m\u001b[39m__\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mand\u001b[39;00m name\u001b[39m.\u001b[39mendswith(\u001b[39m'\u001b[39m\u001b[39m__\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[1;32m    386\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(name)\n\u001b[0;32m--> 387\u001b[0m func \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m\u001b[39m__getitem__\u001b[39;49m(name)\n\u001b[1;32m    388\u001b[0m \u001b[39msetattr\u001b[39m(\u001b[39mself\u001b[39m, name, func)\n\u001b[1;32m    389\u001b[0m \u001b[39mreturn\u001b[39;00m func\n",
      "File \u001b[0;32m/usr/lib/python3.10/ctypes/__init__.py:392\u001b[0m, in \u001b[0;36mCDLL.__getitem__\u001b[0;34m(self, name_or_ordinal)\u001b[0m\n\u001b[1;32m    391\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__getitem__\u001b[39m(\u001b[39mself\u001b[39m, name_or_ordinal):\n\u001b[0;32m--> 392\u001b[0m     func \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_FuncPtr((name_or_ordinal, \u001b[39mself\u001b[39;49m))\n\u001b[1;32m    393\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(name_or_ordinal, \u001b[39mint\u001b[39m):\n\u001b[1;32m    394\u001b[0m         func\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m \u001b[39m=\u001b[39m name_or_ordinal\n",
      "\u001b[0;31mAttributeError\u001b[0m: /home/jhoffbauer/reddit_qa/venv/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cpu.so: undefined symbol: cget_col_row_stats"
     ]
    }
   ],
   "source": [
    "for model in tqdm(models):\n",
    "    results = evaluator.simple_evaluate(\n",
    "        model=\"hf-causal-experimental\",\n",
    "        model_args=f\"pretrained={model},load_in_8bit=True,device_map_option='auto',use_accelerate=True\",\n",
    "        tasks=tasks,\n",
    "        batch_size=\"auto\",\n",
    "        device=\"cuda\",\n",
    "    )\n",
    "\n",
    "\n",
    "    with open(f'eval_results/results-{model.replace(\"/\", \"-\")}-{uuid4().hex}.pkl', 'wb') as f:\n",
    "        pkl.dump(results, f)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
