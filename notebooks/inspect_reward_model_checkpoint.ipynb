{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspecting the reward model checkpoints \n",
    "\n",
    "Loading reward model checkpoints didn't fully work as the `merge_peft_adapter` script cannot load checkpoints in its original form. This explores how checkpoints can potentially be loaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jhoffbauer/reddit_qa/venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2023-08-02 14:30:25.930002: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-08-02 14:30:26.439899: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from dataclasses import dataclass, field\n",
    "from typing import Optional\n",
    "import huggingface_hub\n",
    "import functools as ft\n",
    "import torch\n",
    "import pandas as pd\n",
    "import torch\n",
    "import os\n",
    "from accelerate import Accelerator\n",
    "from datasets import load_dataset\n",
    "from peft import LoraConfig\n",
    "from tqdm import tqdm\n",
    "from transformers import Adafactor, AutoTokenizer, HfArgumentParser, pipeline, AutoConfig, GPTNeoXForCausalLM, AutoModelForCausalLM\n",
    "from trl import AutoModelForCausalLMWithValueHead, PPOConfig, PPOTrainer, set_seed\n",
    "from trl.core import LengthSampler\n",
    "from transformers import pipeline, TextGenerationPipeline, AutoConfig, AutoTokenizer, AutoModelForCausalLM, AutoModel, AutoModelForSequenceClassification, GPTNeoXForCausalLM, LlamaForSequenceClassification\n",
    "from redditqa.dataset import load_reddit_dataset\n",
    "from transformers.utils.hub import convert_file_size_to_int, get_checkpoint_shard_files\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    PeftConfig,\n",
    "    PeftModel,\n",
    "    TaskType,\n",
    "    get_peft_model,\n",
    "    set_peft_model_state_dict,\n",
    ")\n",
    "from peft.utils import _get_submodules\n",
    "import peft\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load and inspect the checkpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's inspect and the checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint = '/scratch1/jhoff/checkpoints/reward_llama-2-7b-chat-hf/checkpoint-3000'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimizer.pt\n",
      "adapter_model.bin\n",
      "scheduler.pt\n",
      "training_args.bin\n",
      "adapter_config.json\n",
      "README.md\n",
      "trainer_state.json\n",
      "rng_state.pth\n"
     ]
    }
   ],
   "source": [
    "print('\\n'.join(os.listdir(model_checkpoint)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's inspect the adapter weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base_model.model.model.layers.0.self_attn.q_proj.lora_A.weight\n",
      "base_model.model.model.layers.0.self_attn.q_proj.lora_B.weight\n",
      "base_model.model.model.layers.0.self_attn.v_proj.lora_A.weight\n",
      "base_model.model.model.layers.0.self_attn.v_proj.lora_B.weight\n",
      "base_model.model.model.layers.1.self_attn.q_proj.lora_A.weight\n",
      "base_model.model.model.layers.1.self_attn.q_proj.lora_B.weight\n",
      "base_model.model.model.layers.1.self_attn.v_proj.lora_A.weight\n",
      "base_model.model.model.layers.1.self_attn.v_proj.lora_B.weight\n",
      "base_model.model.model.layers.2.self_attn.q_proj.lora_A.weight\n",
      "base_model.model.model.layers.2.self_attn.q_proj.lora_B.weight\n",
      "base_model.model.model.layers.2.self_attn.v_proj.lora_A.weight\n",
      "base_model.model.model.layers.2.self_attn.v_proj.lora_B.weight\n",
      "base_model.model.model.layers.3.self_attn.q_proj.lora_A.weight\n",
      "base_model.model.model.layers.3.self_attn.q_proj.lora_B.weight\n",
      "base_model.model.model.layers.3.self_attn.v_proj.lora_A.weight\n",
      "base_model.model.model.layers.3.self_attn.v_proj.lora_B.weight\n",
      "base_model.model.model.layers.4.self_attn.q_proj.lora_A.weight\n",
      "base_model.model.model.layers.4.self_attn.q_proj.lora_B.weight\n",
      "base_model.model.model.layers.4.self_attn.v_proj.lora_A.weight\n",
      "base_model.model.model.layers.4.self_attn.v_proj.lora_B.weight\n",
      "base_model.model.model.layers.5.self_attn.q_proj.lora_A.weight\n",
      "base_model.model.model.layers.5.self_attn.q_proj.lora_B.weight\n",
      "base_model.model.model.layers.5.self_attn.v_proj.lora_A.weight\n",
      "base_model.model.model.layers.5.self_attn.v_proj.lora_B.weight\n",
      "base_model.model.model.layers.6.self_attn.q_proj.lora_A.weight\n",
      "base_model.model.model.layers.6.self_attn.q_proj.lora_B.weight\n",
      "base_model.model.model.layers.6.self_attn.v_proj.lora_A.weight\n",
      "base_model.model.model.layers.6.self_attn.v_proj.lora_B.weight\n",
      "base_model.model.model.layers.7.self_attn.q_proj.lora_A.weight\n",
      "base_model.model.model.layers.7.self_attn.q_proj.lora_B.weight\n",
      "base_model.model.model.layers.7.self_attn.v_proj.lora_A.weight\n",
      "base_model.model.model.layers.7.self_attn.v_proj.lora_B.weight\n",
      "base_model.model.model.layers.8.self_attn.q_proj.lora_A.weight\n",
      "base_model.model.model.layers.8.self_attn.q_proj.lora_B.weight\n",
      "base_model.model.model.layers.8.self_attn.v_proj.lora_A.weight\n",
      "base_model.model.model.layers.8.self_attn.v_proj.lora_B.weight\n",
      "base_model.model.model.layers.9.self_attn.q_proj.lora_A.weight\n",
      "base_model.model.model.layers.9.self_attn.q_proj.lora_B.weight\n",
      "base_model.model.model.layers.9.self_attn.v_proj.lora_A.weight\n",
      "base_model.model.model.layers.9.self_attn.v_proj.lora_B.weight\n",
      "base_model.model.model.layers.10.self_attn.q_proj.lora_A.weight\n",
      "base_model.model.model.layers.10.self_attn.q_proj.lora_B.weight\n",
      "base_model.model.model.layers.10.self_attn.v_proj.lora_A.weight\n",
      "base_model.model.model.layers.10.self_attn.v_proj.lora_B.weight\n",
      "base_model.model.model.layers.11.self_attn.q_proj.lora_A.weight\n",
      "base_model.model.model.layers.11.self_attn.q_proj.lora_B.weight\n",
      "base_model.model.model.layers.11.self_attn.v_proj.lora_A.weight\n",
      "base_model.model.model.layers.11.self_attn.v_proj.lora_B.weight\n",
      "base_model.model.model.layers.12.self_attn.q_proj.lora_A.weight\n",
      "base_model.model.model.layers.12.self_attn.q_proj.lora_B.weight\n",
      "base_model.model.model.layers.12.self_attn.v_proj.lora_A.weight\n",
      "base_model.model.model.layers.12.self_attn.v_proj.lora_B.weight\n",
      "base_model.model.model.layers.13.self_attn.q_proj.lora_A.weight\n",
      "base_model.model.model.layers.13.self_attn.q_proj.lora_B.weight\n",
      "base_model.model.model.layers.13.self_attn.v_proj.lora_A.weight\n",
      "base_model.model.model.layers.13.self_attn.v_proj.lora_B.weight\n",
      "base_model.model.model.layers.14.self_attn.q_proj.lora_A.weight\n",
      "base_model.model.model.layers.14.self_attn.q_proj.lora_B.weight\n",
      "base_model.model.model.layers.14.self_attn.v_proj.lora_A.weight\n",
      "base_model.model.model.layers.14.self_attn.v_proj.lora_B.weight\n",
      "base_model.model.model.layers.15.self_attn.q_proj.lora_A.weight\n",
      "base_model.model.model.layers.15.self_attn.q_proj.lora_B.weight\n",
      "base_model.model.model.layers.15.self_attn.v_proj.lora_A.weight\n",
      "base_model.model.model.layers.15.self_attn.v_proj.lora_B.weight\n",
      "base_model.model.model.layers.16.self_attn.q_proj.lora_A.weight\n",
      "base_model.model.model.layers.16.self_attn.q_proj.lora_B.weight\n",
      "base_model.model.model.layers.16.self_attn.v_proj.lora_A.weight\n",
      "base_model.model.model.layers.16.self_attn.v_proj.lora_B.weight\n",
      "base_model.model.model.layers.17.self_attn.q_proj.lora_A.weight\n",
      "base_model.model.model.layers.17.self_attn.q_proj.lora_B.weight\n",
      "base_model.model.model.layers.17.self_attn.v_proj.lora_A.weight\n",
      "base_model.model.model.layers.17.self_attn.v_proj.lora_B.weight\n",
      "base_model.model.model.layers.18.self_attn.q_proj.lora_A.weight\n",
      "base_model.model.model.layers.18.self_attn.q_proj.lora_B.weight\n",
      "base_model.model.model.layers.18.self_attn.v_proj.lora_A.weight\n",
      "base_model.model.model.layers.18.self_attn.v_proj.lora_B.weight\n",
      "base_model.model.model.layers.19.self_attn.q_proj.lora_A.weight\n",
      "base_model.model.model.layers.19.self_attn.q_proj.lora_B.weight\n",
      "base_model.model.model.layers.19.self_attn.v_proj.lora_A.weight\n",
      "base_model.model.model.layers.19.self_attn.v_proj.lora_B.weight\n",
      "base_model.model.model.layers.20.self_attn.q_proj.lora_A.weight\n",
      "base_model.model.model.layers.20.self_attn.q_proj.lora_B.weight\n",
      "base_model.model.model.layers.20.self_attn.v_proj.lora_A.weight\n",
      "base_model.model.model.layers.20.self_attn.v_proj.lora_B.weight\n",
      "base_model.model.model.layers.21.self_attn.q_proj.lora_A.weight\n",
      "base_model.model.model.layers.21.self_attn.q_proj.lora_B.weight\n",
      "base_model.model.model.layers.21.self_attn.v_proj.lora_A.weight\n",
      "base_model.model.model.layers.21.self_attn.v_proj.lora_B.weight\n",
      "base_model.model.model.layers.22.self_attn.q_proj.lora_A.weight\n",
      "base_model.model.model.layers.22.self_attn.q_proj.lora_B.weight\n",
      "base_model.model.model.layers.22.self_attn.v_proj.lora_A.weight\n",
      "base_model.model.model.layers.22.self_attn.v_proj.lora_B.weight\n",
      "base_model.model.model.layers.23.self_attn.q_proj.lora_A.weight\n",
      "base_model.model.model.layers.23.self_attn.q_proj.lora_B.weight\n",
      "base_model.model.model.layers.23.self_attn.v_proj.lora_A.weight\n",
      "base_model.model.model.layers.23.self_attn.v_proj.lora_B.weight\n",
      "base_model.model.model.layers.24.self_attn.q_proj.lora_A.weight\n",
      "base_model.model.model.layers.24.self_attn.q_proj.lora_B.weight\n",
      "base_model.model.model.layers.24.self_attn.v_proj.lora_A.weight\n",
      "base_model.model.model.layers.24.self_attn.v_proj.lora_B.weight\n",
      "base_model.model.model.layers.25.self_attn.q_proj.lora_A.weight\n",
      "base_model.model.model.layers.25.self_attn.q_proj.lora_B.weight\n",
      "base_model.model.model.layers.25.self_attn.v_proj.lora_A.weight\n",
      "base_model.model.model.layers.25.self_attn.v_proj.lora_B.weight\n",
      "base_model.model.model.layers.26.self_attn.q_proj.lora_A.weight\n",
      "base_model.model.model.layers.26.self_attn.q_proj.lora_B.weight\n",
      "base_model.model.model.layers.26.self_attn.v_proj.lora_A.weight\n",
      "base_model.model.model.layers.26.self_attn.v_proj.lora_B.weight\n",
      "base_model.model.model.layers.27.self_attn.q_proj.lora_A.weight\n",
      "base_model.model.model.layers.27.self_attn.q_proj.lora_B.weight\n",
      "base_model.model.model.layers.27.self_attn.v_proj.lora_A.weight\n",
      "base_model.model.model.layers.27.self_attn.v_proj.lora_B.weight\n",
      "base_model.model.model.layers.28.self_attn.q_proj.lora_A.weight\n",
      "base_model.model.model.layers.28.self_attn.q_proj.lora_B.weight\n",
      "base_model.model.model.layers.28.self_attn.v_proj.lora_A.weight\n",
      "base_model.model.model.layers.28.self_attn.v_proj.lora_B.weight\n",
      "base_model.model.model.layers.29.self_attn.q_proj.lora_A.weight\n",
      "base_model.model.model.layers.29.self_attn.q_proj.lora_B.weight\n",
      "base_model.model.model.layers.29.self_attn.v_proj.lora_A.weight\n",
      "base_model.model.model.layers.29.self_attn.v_proj.lora_B.weight\n",
      "base_model.model.model.layers.30.self_attn.q_proj.lora_A.weight\n",
      "base_model.model.model.layers.30.self_attn.q_proj.lora_B.weight\n",
      "base_model.model.model.layers.30.self_attn.v_proj.lora_A.weight\n",
      "base_model.model.model.layers.30.self_attn.v_proj.lora_B.weight\n",
      "base_model.model.model.layers.31.self_attn.q_proj.lora_A.weight\n",
      "base_model.model.model.layers.31.self_attn.q_proj.lora_B.weight\n",
      "base_model.model.model.layers.31.self_attn.v_proj.lora_A.weight\n",
      "base_model.model.model.layers.31.self_attn.v_proj.lora_B.weight\n",
      "base_model.model.score.weight\n"
     ]
    }
   ],
   "source": [
    "adapter_weights = torch.load(f'{model_checkpoint}/adapter_model.bin', map_location='cpu')\n",
    "print('\\n'.join(list(adapter_weights.keys())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The adapter config. (When creating an adapter to load the weights, we should use the exact same config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"auto_mapping\": null,\n",
      "  \"base_model_name_or_path\": \"meta-llama/Llama-2-7b-chat-hf\",\n",
      "  \"bias\": \"none\",\n",
      "  \"fan_in_fan_out\": false,\n",
      "  \"inference_mode\": true,\n",
      "  \"init_lora_weights\": true,\n",
      "  \"layers_pattern\": null,\n",
      "  \"layers_to_transform\": null,\n",
      "  \"lora_alpha\": 32,\n",
      "  \"lora_dropout\": 0.1,\n",
      "  \"modules_to_save\": null,\n",
      "  \"peft_type\": \"LORA\",\n",
      "  \"r\": 8,\n",
      "  \"revision\": null,\n",
      "  \"target_modules\": [\n",
      "    \"q_proj\",\n",
      "    \"v_proj\"\n",
      "  ],\n",
      "  \"task_type\": \"SEQ_CLS\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(open(os.path.join(model_checkpoint, 'adapter_config.json')).read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have the model weights and weights for the scoring head. Inspect the scoring head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 4096]), tensor(0.0381, dtype=torch.bfloat16))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adapter_weights['base_model.model.score.weight'].shape, adapter_weights['base_model.model.score.weight'][0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems like the scoring head is not an adapter but an actual weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's load the actual LLAMA 2 model that is the base for our adapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.30it/s]\n",
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at meta-llama/Llama-2-7b-chat-hf and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "base_model_name = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "\n",
    "# Use initializer_range = 0 to make sure initialized weights are 0\n",
    "model = AutoModelForSequenceClassification.from_pretrained(base_model_name, num_labels=1, initializer_range=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForSequenceClassification(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(32000, 4096)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm()\n",
       "  )\n",
       "  (score): Linear(in_features=4096, out_features=1, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[0., 0., 0.,  ..., 0., 0., 0.]], requires_grad=True)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.score.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_config = LoraConfig(\n",
    "    task_type=TaskType.SEQ_CLS,\n",
    "    inference_mode=False,\n",
    "    r=8,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.1,\n",
    ")\n",
    "model_peft = get_peft_model(model, peft_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModelForSequenceClassification(\n",
       "  (base_model): LoraModel(\n",
       "    (model): LlamaForSequenceClassification(\n",
       "      (model): LlamaModel(\n",
       "        (embed_tokens): Embedding(32000, 4096)\n",
       "        (layers): ModuleList(\n",
       "          (0-31): 32 x LlamaDecoderLayer(\n",
       "            (self_attn): LlamaAttention(\n",
       "              (q_proj): Linear(\n",
       "                in_features=4096, out_features=4096, bias=False\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "              (v_proj): Linear(\n",
       "                in_features=4096, out_features=4096, bias=False\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "              (rotary_emb): LlamaRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): LlamaMLP(\n",
       "              (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "              (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "              (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "              (act_fn): SiLUActivation()\n",
       "            )\n",
       "            (input_layernorm): LlamaRMSNorm()\n",
       "            (post_attention_layernorm): LlamaRMSNorm()\n",
       "          )\n",
       "        )\n",
       "        (norm): LlamaRMSNorm()\n",
       "      )\n",
       "      (score): ModulesToSaveWrapper(\n",
       "        (original_module): Linear(in_features=4096, out_features=1, bias=False)\n",
       "        (modules_to_save): ModuleDict(\n",
       "          (default): Linear(in_features=4096, out_features=1, bias=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_peft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[0., 0., 0.,  ..., 0., 0., 0.]], requires_grad=True)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_peft.score.modules_to_save.default.weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we bring in the adapter weights from the checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_peft_model_state_dict(model_peft, adapter_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0381, grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_peft.score.modules_to_save.default.weight[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we see that the score weight is correct as it matches the value from above! ✅"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Merge the peft model into a normal model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_out = model_peft.merge_and_unload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0381)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_out.score.weight.data[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('/scratch1/jhoff/tmp/reward_model/tokenizer_config.json',\n",
       " '/scratch1/jhoff/tmp/reward_model/special_tokens_map.json',\n",
       " '/scratch1/jhoff/tmp/reward_model/tokenizer.model',\n",
       " '/scratch1/jhoff/tmp/reward_model/added_tokens.json',\n",
       " '/scratch1/jhoff/tmp/reward_model/tokenizer.json')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_name = \"/scratch1/jhoff/tmp/reward_model\"\n",
    "model_out.save_pretrained(output_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n",
    "tokenizer.save_pretrained(output_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks about right! ✅"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate the reward model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /scratch1/jhoff/reddit_dataset_cached/eval/cache-cba55e4212677d14.arrow\n",
      "Loading cached processed dataset at /scratch1/jhoff/reddit_dataset_cached/eval/cache-d8898fc7c787d1eb.arrow\n",
      "Loading cached shuffled indices for dataset at /scratch1/jhoff/reddit_dataset_cached/eval/cache-e35089f0b695ca2b.arrow\n",
      "Loading cached shuffled indices for dataset at /scratch1/jhoff/reddit_dataset_cached/eval/cache-ea21b592f4358562.arrow\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:08<00:00,  2.81s/it]\n"
     ]
    }
   ],
   "source": [
    "from redditqa.dataset import load_reddit_dataset\n",
    "\n",
    "eval_dataset = load_reddit_dataset(\"eval\", pairs=True)\n",
    "eval_dataset = eval_dataset.shuffle(seed=42).select(range(1000))\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n",
    "\n",
    "reward_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    output_name, \n",
    "    num_labels=1, \n",
    "    torch_dtype=torch.bfloat16\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "reward_pipe = pipeline(\n",
    "    \"sentiment-analysis\",\n",
    "    model=reward_model,\n",
    "    tokenizer=tokenizer,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter 'function'=<function apply_reward_model at 0x7fc2e833c040> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n",
      "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   0%|          | 5/1000 [00:01<04:07,  4.02 examples/s]/home/jhoffbauer/reddit_qa/venv/lib/python3.10/site-packages/transformers/pipelines/base.py:1082: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n",
      "                                                              \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'answer_link_id': '2y0dxt',\n",
       " 'question_title': 'Why do employers ask \"where do you see yourself in 5-10 years?\" How do personal goals matter at all?',\n",
       " 'response_j': 'After you respond, be sure to ask, \"How do you see the company changing over that timespan?\"',\n",
       " 'response_k': \"Whatever job you're applying for, think of the logical career path and where you should be in fifteen years. Like all basic interview questions it's more about whether you can have an adult conversation than the actual answers. \",\n",
       " 'score_j': 13,\n",
       " 'score_k': 2,\n",
       " 'reward_j': 0.8872045874595642,\n",
       " 'reward_k': 0.8568122982978821}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "template = \"<|ELIF|> Question: %question\\nAnswer: %answer\"\n",
    "\n",
    "def apply_reward_model(row):\n",
    "\n",
    "    question_title = row[\"question_title\"]\n",
    "    response_j = row[\"response_j\"]\n",
    "    response_k = row[\"response_k\"]\n",
    "\n",
    "    qa_j = template.replace(\"%question\", question_title).replace(\"%answer\", response_j)\n",
    "    reward_j = reward_pipe(qa_j)[0][\"score\"]\n",
    "\n",
    "    qa_k = template.replace(\"%question\", question_title).replace(\"%answer\", response_k)\n",
    "    reward_k = reward_pipe(qa_k)[0][\"score\"]\n",
    "\n",
    "    return {\n",
    "        'reward_j': float(reward_j),\n",
    "        'reward_k': float(reward_k),\n",
    "    }\n",
    "\n",
    "eval_dataset = eval_dataset.map(apply_reward_model)\n",
    "\n",
    "eval_dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's measure the accuracy of the reward model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.644\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "for row in eval_dataset:\n",
    "    if row['reward_j'] >= row['reward_k']:\n",
    "        correct += 1\n",
    "\n",
    "print(f'Accuracy: {correct / len(eval_dataset)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That looks about right!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
