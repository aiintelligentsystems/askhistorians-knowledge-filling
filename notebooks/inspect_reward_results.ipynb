{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jhoffbauer/reddit_qa/venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2023-07-30 15:54:39.039729: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-07-30 15:54:39.576292: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from dataclasses import dataclass, field\n",
    "from typing import Optional\n",
    "import huggingface_hub\n",
    "import functools as ft\n",
    "import torch\n",
    "import pandas as pd\n",
    "import torch\n",
    "from accelerate import Accelerator\n",
    "from datasets import load_dataset\n",
    "from peft import LoraConfig\n",
    "from tqdm import tqdm\n",
    "from transformers import Adafactor, AutoTokenizer, HfArgumentParser, pipeline, AutoConfig, GPTNeoXForCausalLM, AutoModelForCausalLM\n",
    "from trl import AutoModelForCausalLMWithValueHead, PPOConfig, PPOTrainer, set_seed\n",
    "from trl.core import LengthSampler\n",
    "from transformers import pipeline, TextGenerationPipeline, AutoConfig, AutoTokenizer, AutoModelForCausalLM, AutoModel, AutoModelForSequenceClassification, GPTNeoXForCausalLM, LlamaForSequenceClassification\n",
    "from redditqa.dataset import load_reddit_dataset\n",
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /scratch1/jhoff/reddit_dataset_cached/eval/cache-cba55e4212677d14.arrow\n",
      "Loading cached processed dataset at /scratch1/jhoff/reddit_dataset_cached/eval/cache-d8898fc7c787d1eb.arrow\n",
      "Loading cached shuffled indices for dataset at /scratch1/jhoff/reddit_dataset_cached/eval/cache-e35089f0b695ca2b.arrow\n",
      "Loading cached shuffled indices for dataset at /scratch1/jhoff/reddit_dataset_cached/eval/cache-ea21b592f4358562.arrow\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(Dataset({\n",
       "     features: ['answer_link_id', 'question_title', 'response_j', 'response_k', 'score_j', 'score_k'],\n",
       "     num_rows: 1000\n",
       " }),\n",
       " {'answer_link_id': '2y0dxt',\n",
       "  'question_title': 'Why do employers ask \"where do you see yourself in 5-10 years?\" How do personal goals matter at all?',\n",
       "  'response_j': 'After you respond, be sure to ask, \"How do you see the company changing over that timespan?\"',\n",
       "  'response_k': \"Whatever job you're applying for, think of the logical career path and where you should be in fifteen years. Like all basic interview questions it's more about whether you can have an adult conversation than the actual answers. \",\n",
       "  'score_j': 13,\n",
       "  'score_k': 2})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the dataset \n",
    "\n",
    "eval_dataset = load_reddit_dataset(\"eval\", pairs=True)\n",
    "eval_dataset = eval_dataset.shuffle(seed=42).select(range(1000))\n",
    "\n",
    "eval_dataset, eval_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]/home/jhoffbauer/reddit_qa/venv/lib/python3.10/site-packages/transformers/pipelines/base.py:1082: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n",
      "                                                               \r"
     ]
    }
   ],
   "source": [
    "# Add sentiment analysis and question detection to both answers \n",
    "\n",
    "sentiment_pipe = pipeline(\"text-classification\", model=\"michellejieli/emotion_text_classifier\", device='cuda:0')\n",
    "question_detection_pipe = pipeline(\"text-classification\", model=\"huaen/question_detection\", device='cuda:0')\n",
    "\n",
    "def apply_get_emotions(row): \n",
    "    row['emotion_j'] = sentiment_pipe(row['response_j'])[0]['label']\n",
    "    row['emotion_k'] = sentiment_pipe(row['response_k'])[0]['label']\n",
    "    return row\n",
    "\n",
    "def apply_question_detection(row): \n",
    "    row['is_question_j'] = question_detection_pipe(row['response_j'])[0]['label']\n",
    "    row['is_question_k'] = question_detection_pipe(row['response_k'])[0]['label']\n",
    "    return row\n",
    "    \n",
    "eval_dataset = eval_dataset.map(apply_get_emotions).map(apply_question_detection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's free up some cuda memory\n",
    "\n",
    "del sentiment_pipe\n",
    "del question_detection_pipe\n",
    "\n",
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /scratch1/jhoff/reddit_dataset_cached/eval/cache-55f17661bd97c21a.arrow\n"
     ]
    }
   ],
   "source": [
    "def percent(x): \n",
    "    return \"{:.2f}%\".format(x * 100)\n",
    "\n",
    "def apply_textblob(row):\n",
    "    textblob_j = TextBlob(row[\"response_j\"]).sentiment\n",
    "    textblob_k = TextBlob(row[\"response_k\"]).sentiment\n",
    "    row[\"polarity_j\"] = percent(textblob_j.polarity)\n",
    "    row[\"subjectivity_j\"] = percent(textblob_j.subjectivity)\n",
    "    row[\"polarity_k\"] = percent(textblob_k.polarity)\n",
    "    row[\"subjectivity_k\"] = percent(textblob_k.subjectivity)\n",
    "    return row\n",
    "\n",
    "eval_dataset = eval_dataset.map(apply_textblob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint = '/scratch1/jhoff/checkpoints/reward_llama-2-7b-chat-hf/checkpoint-3000_merged'\n",
    "\n",
    "model_config = AutoConfig.from_pretrained(model_checkpoint)\n",
    "model_config.architectures  = ['LlamaForSequenceClassification']\n",
    "model_config.num_labels = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaConfig {\n",
       "  \"_name_or_path\": \"/scratch1/jhoff/checkpoints/reward_llama-2-7b-chat-hf/checkpoint-3000_merged\",\n",
       "  \"architectures\": [\n",
       "    \"LlamaForSequenceClassification\"\n",
       "  ],\n",
       "  \"bos_token_id\": 1,\n",
       "  \"eos_token_id\": 2,\n",
       "  \"hidden_act\": \"silu\",\n",
       "  \"hidden_size\": 4096,\n",
       "  \"id2label\": {\n",
       "    \"0\": \"LABEL_0\"\n",
       "  },\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 11008,\n",
       "  \"label2id\": {\n",
       "    \"LABEL_0\": 0\n",
       "  },\n",
       "  \"max_position_embeddings\": 4096,\n",
       "  \"model_type\": \"llama\",\n",
       "  \"num_attention_heads\": 32,\n",
       "  \"num_hidden_layers\": 32,\n",
       "  \"num_key_value_heads\": 32,\n",
       "  \"pad_token_id\": 0,\n",
       "  \"pretraining_tp\": 1,\n",
       "  \"rms_norm_eps\": 1e-06,\n",
       "  \"rope_scaling\": null,\n",
       "  \"tie_word_embeddings\": false,\n",
       "  \"torch_dtype\": \"float32\",\n",
       "  \"transformers_version\": \"4.32.0.dev0\",\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 32000\n",
       "}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:08<00:00,  2.85s/it]\n",
      "Some weights of the model checkpoint at /scratch1/jhoff/checkpoints/reward_llama-2-7b-chat-hf/checkpoint-3000_merged were not used when initializing LlamaForSequenceClassification: ['score.modules_to_save.default.weight', 'score.original_module.weight']\n",
      "- This IS expected if you are initializing LlamaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing LlamaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at /scratch1/jhoff/checkpoints/reward_llama-2-7b-chat-hf/checkpoint-3000_merged and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "reward_model = LlamaForSequenceClassification.from_pretrained(model_checkpoint, torch_dtype=torch.bfloat16, config=model_config)\n",
    "#reward_model = LlamaForSequenceClassification.from_pretrained(model_checkpoint, torch_dtype=torch.bfloat16, num_labels=1)\n",
    "#reward_model.cuda()\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_checkpoint_unmerged = model_checkpoint.replace(\"_merged\", \"\")\n",
    "#adapter_weights = torch.load(f'{model_checkpoint_unmerged}/adapter_model.bin', map_location='cpu')\n",
    "#reward_model.score.weight.data = adapter_weights['base_model.model.score.weight']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "reward_pipe = pipeline(\n",
    "    \"sentiment-analysis\",\n",
    "    model=reward_model,\n",
    "    tokenizer=tokenizer,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.0085,  0.0123,  0.0075,  ...,  0.0145,  0.0075, -0.0142]],\n",
       "       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reward_pipe.model.score.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter 'function'=<function apply_reward_model at 0x7f82377056c0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n",
      "Map:   0%|          | 5/1000 [00:00<02:16,  7.31 examples/s]/home/jhoffbauer/reddit_qa/venv/lib/python3.10/site-packages/transformers/pipelines/base.py:1082: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n",
      "                                                              \r"
     ]
    }
   ],
   "source": [
    "template = \"<|ELIF|> Question: %question\\nAnswer: %answer\"\n",
    "\n",
    "def apply_reward_model(row):\n",
    "\n",
    "    question_title = row[\"question_title\"]\n",
    "    response_j = row[\"response_j\"]\n",
    "    response_k = row[\"response_k\"]\n",
    "\n",
    "    qa_j = template.replace(\"%question\", question_title).replace(\"%answer\", response_j)\n",
    "    reward_j = reward_pipe(qa_j)[0][\"score\"]\n",
    "\n",
    "    qa_k = template.replace(\"%question\", question_title).replace(\"%answer\", response_k)\n",
    "    reward_k = reward_pipe(qa_k)[0][\"score\"]\n",
    "\n",
    "    return {\n",
    "        'reward_j': float(reward_j),\n",
    "        'reward_k': float(reward_k),\n",
    "    }\n",
    "\n",
    "eval_dataset = eval_dataset.map(apply_reward_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'answer_link_id': '2y0dxt',\n",
       " 'question_title': 'Why do employers ask \"where do you see yourself in 5-10 years?\" How do personal goals matter at all?',\n",
       " 'response_j': 'After you respond, be sure to ask, \"How do you see the company changing over that timespan?\"',\n",
       " 'response_k': \"Whatever job you're applying for, think of the logical career path and where you should be in fifteen years. Like all basic interview questions it's more about whether you can have an adult conversation than the actual answers. \",\n",
       " 'score_j': 13,\n",
       " 'score_k': 2,\n",
       " 'reward_j': 0.9744347929954529,\n",
       " 'reward_k': 0.5580862164497375}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.503\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "for row in eval_dataset:\n",
    "    if row['reward_j'] >= row['reward_k']:\n",
    "        correct += 1\n",
    "\n",
    "print(f'Accuracy: {correct / len(eval_dataset)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
