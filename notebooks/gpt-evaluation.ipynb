{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import random\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "# %%\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Optional\n",
    "\n",
    "import huggingface_hub\n",
    "from openai import OpenAI\n",
    "import torch\n",
    "from accelerate import Accelerator\n",
    "from datasets import load_dataset\n",
    "import datasets as ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['question_created_utc', 'question_retrieved_on', 'question_deleted', 'question_title', 'question_selftext', 'question_score', 'question_char_length', 'question_selftext_char_length', 'answers', 'graded_output', 'Zephyr-7B-beta', 'Zephyr-History-7600'],\n",
       "    num_rows: 100\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = ds.load_from_disk(\"/scratch1/redditqa/cached_datasets/AskHistorians_test_set_100_model_generated\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_a = 'Zephyr-7B-beta'\n",
    "model_b = 'Zephyr-History-7600'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_order(models):\n",
    "    return random.sample(models, k=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"You are a seasoned historian tasked with evaluating responses to historical questions. Consider the following question and assess which of the two provided answers presents the most accurate and comprehensive information. \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT_TEMPLATE = \"\"\"Question: %QUESTION\n",
    "\n",
    "Answer 1: %ANSWER1\n",
    "\n",
    "Answer 2: %ANSWER2\n",
    "\n",
    "Output your final verdict by strictly following this format: \"[[1]]\" if answer 1 is better, \"[[2]]\"\n",
    "if answer 2 is better.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI(api_key=\"your_key\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gpt4_preference(ds_item):\n",
    "    model1, model2 = get_model_order([model_a, model_b])\n",
    "    user_prompt = PROMPT_TEMPLATE.replace(\"%QUESTION\", ds_item['question_title'])\n",
    "    user_prompt = user_prompt.replace(\"%ANSWER1\", ds_item[model1])\n",
    "    user_prompt = user_prompt.replace(\"%ANSWER2\", ds_item[model2])\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "    model=\"gpt-4-1106-preview\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ],\n",
    "        temperature=0.0,\n",
    "        max_tokens=5,\n",
    "        top_p=1,\n",
    "        frequency_penalty=0,\n",
    "        presence_penalty=0,\n",
    "        stop=[\"\\n\"],\n",
    "    )\n",
    "    answer = response.choices[0].message.content\n",
    "\n",
    "    ds_item['model-order'] = f\"Answer1:{model1};Answer2:{model2}\"\n",
    "    ds_item['raw-gpt4-answer'] = answer\n",
    "\n",
    "    # Convert to preference\n",
    "    if \"1\" in answer and \"2\" in answer:\n",
    "        ds_item['gpt4-preference'] = \"\"\n",
    "    elif \"1\" in answer:\n",
    "        ds_item['gpt4-preference'] = model1\n",
    "    elif \"2\" in answer:\n",
    "        ds_item['gpt4-preference'] = model2\n",
    "    else:\n",
    "        ds_item['gpt4-preference'] = \"\"\n",
    "\n",
    "    return ds_item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter 'function'=<function gpt4_preference at 0x7f105b528940> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n",
      "Map:   0%|          | 0/100 [00:00<?, ? examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 100/100 [01:30<00:00,  1.10 examples/s]\n"
     ]
    }
   ],
   "source": [
    "dataset = dataset.map(gpt4_preference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 100/100 [00:00<00:00, 14270.71 examples/s]\n"
     ]
    }
   ],
   "source": [
    "dataset.save_to_disk(\"/scratch1/redditqa/cached_datasets/AskHistorians_test_set_100_model_generated_gpt4_preference\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'Zephyr-7B-beta': 94, 'Zephyr-History-7600': 5, '': 1})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(dataset['gpt4-preference'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
