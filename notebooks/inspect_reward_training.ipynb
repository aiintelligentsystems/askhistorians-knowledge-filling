{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Any, Dict, List, Optional, Union\n",
    "\n",
    "import evaluate\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from accelerate import Accelerator, infer_auto_device_map, init_empty_weights\n",
    "from datasets import load_dataset\n",
    "from huggingface_hub import login\n",
    "from peft import LoraConfig, TaskType, get_peft_model\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoConfig,\n",
    "    AutoModelForCausalLM,\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoTokenizer,\n",
    "    HfArgumentParser,\n",
    "    LlamaForCausalLM,\n",
    "    LlamaTokenizer,\n",
    "    PreTrainedTokenizerBase,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    logging,\n",
    "    set_seed,\n",
    ")\n",
    "from transformers.utils import PaddingStrategy\n",
    "from trl import SFTTrainer\n",
    "from trl.trainer import ConstantLengthDataset\n",
    "\n",
    "from redditqa.dataset import load_reddit_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid.\n",
      "Your token has been saved to /root/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "# Login to the HuggingFace Hub\n",
    "HUGGINGFACE_TOKEN = os.environ.get(\"HUGGINGFACE_TOKEN\", None)\n",
    "if HUGGINGFACE_TOKEN is not None:\n",
    "    login(token=HUGGINGFACE_TOKEN)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'meta-llama/Llama-2-7b-chat-hf'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /scratch1/jhoff/reddit_dataset_cached/train/cache-757f0ee80690267b.arrow\n",
      "Loading cached processed dataset at /scratch1/jhoff/reddit_dataset_cached/train/cache-ec6a7572749a5bef.arrow\n",
      "Loading cached shuffled indices for dataset at /scratch1/jhoff/reddit_dataset_cached/train/cache-fcce8e9f7c96b1b8.arrow\n",
      "Loading cached processed dataset at /scratch1/jhoff/reddit_dataset_cached/eval/cache-cba55e4212677d14.arrow\n",
      "Loading cached processed dataset at /scratch1/jhoff/reddit_dataset_cached/eval/cache-d8898fc7c787d1eb.arrow\n",
      "Loading cached shuffled indices for dataset at /scratch1/jhoff/reddit_dataset_cached/eval/cache-e35089f0b695ca2b.arrow\n"
     ]
    }
   ],
   "source": [
    "train_dataset = load_reddit_dataset(\"train\", pairs=True)\n",
    "eval_dataset = load_reddit_dataset(\"eval\", pairs=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['answer_link_id', 'question_title', 'response_j', 'response_k', 'score_j', 'score_k'])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /scratch1/jhoff/reddit_dataset_cached/train/cache-b84fa26c2fd2d3bf.arrow\n",
      "Loading cached processed dataset at /scratch1/jhoff/reddit_dataset_cached/eval/cache-215276af46097339.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished preprocessing dataset.\n",
      "Number of training examples:  110865\n",
      "Number of eval examples:  47799\n"
     ]
    }
   ],
   "source": [
    "# Turn the dataset into pairs of post + summaries, where text_j is the preferred question + answer and text_k is the other.\n",
    "# Then tokenize the dataset.\n",
    "def preprocess_function(examples):\n",
    "    new_examples = {\n",
    "        \"input_ids_j\": [],\n",
    "        \"attention_mask_j\": [],\n",
    "        \"score_j\": [],\n",
    "        \"input_ids_k\": [],\n",
    "        \"attention_mask_k\": [],\n",
    "        \"score_k\": [],\n",
    "    }\n",
    "    for question_title, response_j, response_k, score_j, score_k in zip(\n",
    "        examples[\"question_title\"], examples[\"response_j\"], examples[\"response_k\"], examples[\"score_j\"], examples[\"score_k\"]\n",
    "    ):\n",
    "        template = \"<|ELIF|> Question: %question\\nAnswer: %answer\"\n",
    "\n",
    "        text_j = template.replace(\"%question\", question_title).replace(\"%answer\", response_j)\n",
    "        text_k = template.replace(\"%question\", question_title).replace(\"%answer\", response_k)\n",
    "        tokenized_j = tokenizer(text_j, truncation=True)\n",
    "        tokenized_k = tokenizer(text_k, truncation=True)\n",
    "\n",
    "        new_examples[\"input_ids_j\"].append(tokenized_j[\"input_ids\"])\n",
    "        new_examples[\"attention_mask_j\"].append(tokenized_j[\"attention_mask\"])\n",
    "        new_examples[\"score_j\"].append(score_j)\n",
    "        new_examples[\"input_ids_k\"].append(tokenized_k[\"input_ids\"])\n",
    "        new_examples[\"attention_mask_k\"].append(tokenized_k[\"attention_mask\"])\n",
    "        new_examples[\"score_k\"].append(score_k)\n",
    "\n",
    "    return new_examples\n",
    "\n",
    "\n",
    "original_columns = train_dataset.column_names\n",
    "\n",
    "# Preprocess the dataset\n",
    "train_dataset_preproc = train_dataset.map(preprocess_function, num_proc=1, remove_columns=original_columns, batched=True)\n",
    "eval_dataset_preproc = eval_dataset.map(preprocess_function, num_proc=1, remove_columns=original_columns, batched=True)\n",
    "\n",
    "# # Filter \n",
    "# train_dataset = train_dataset.filter(\n",
    "#     lambda x: len(x[\"input_ids_j\"]) <= script_args.max_length and len(x[\"input_ids_k\"]) <= script_args.max_length\n",
    "# )\n",
    "# eval_dataset = eval_dataset.filter(\n",
    "#     lambda x: len(x[\"input_ids_j\"]) <= script_args.max_length and len(x[\"input_ids_k\"]) <= script_args.max_length\n",
    "# )\n",
    "\n",
    "# # Shuffle and select a subset of the dataset\n",
    "# eval_dataset_preproc = eval_dataset_preproc.shuffle(seed=SEED).select(range(script_args.eval_subsample))\n",
    "\n",
    "print(\"Finished preprocessing dataset.\")\n",
    "print(\"Number of training examples: \", len(train_dataset))\n",
    "print(\"Number of eval examples: \", len(eval_dataset))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['score_j', 'score_k', 'input_ids_j', 'attention_mask_j', 'input_ids_k', 'attention_mask_k'])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset_preproc[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
