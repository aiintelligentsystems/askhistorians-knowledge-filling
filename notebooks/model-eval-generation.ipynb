{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2023-12-17 12:44:05.576685: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2023-12-17 12:44:05.576711: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2023-12-17 12:44:05.577774: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-12-17 12:44:05.582607: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-12-17 12:44:06.318390: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "# external\n",
    "import datasets as ds\n",
    "from transformers import pipeline\n",
    "import importlib\n",
    "import os\n",
    "import torch\n",
    "import transformers\n",
    "from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from functools import partial\n",
    "import numpy as np\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    HfArgumentParser,\n",
    "    TrainingArguments,\n",
    "    set_seed,\n",
    "    TextGenerationPipeline,\n",
    "    GenerationConfig\n",
    ")\n",
    "import tqdm\n",
    "from functools import partial\n",
    "\n",
    "# internal\n",
    "from redditqa.data.smart_filter import question_grading_map, question_filter, answer_grading_map, answer_filter\n",
    "from redditqa.data import qa_generation\n",
    "from redditqa.data.util import mask_links\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set_seed(42)\n",
    "# # Load the dataset\n",
    "# # dataset_dict = load_reddit_dataset(pairs=False)\n",
    "# dataset = ds.load_from_disk(\"/scratch1/redditqa/cached_datasets/AskHistorians_question_filtered.jsonl\")\n",
    "# question_filter_func = partial(question_filter, accepted_token_str=[\"y\", \"yes\"])\n",
    "# dataset = dataset.filter(question_filter_func)\n",
    "\n",
    "# test_data = dataset.train_test_split(test_size=0.1)[\"test\"]\n",
    "# test_data_100 = test_data.select(list(range(100)))\n",
    "# test_data_100.save_to_disk(\"/scratch1/redditqa/cached_datasets/AskHistorians_test_set_100\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['question_created_utc', 'question_retrieved_on', 'question_deleted', 'question_title', 'question_selftext', 'question_score', 'question_char_length', 'question_selftext_char_length', 'answers', 'graded_output'],\n",
       "    num_rows: 100\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = ds.load_from_disk(\"/scratch1/redditqa/cached_datasets/AskHistorians_test_set_100\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoints = {\n",
    "    'Zephyr-7B-beta': 'HuggingFaceH4/zephyr-7b-beta',\n",
    "    'Zephyr-History-7600': '/scratch1/redditqa/ws23/zephyr_dpo_filtered_dataset/checkpoint-7600_merged',\n",
    "}\n",
    "\n",
    "cache_dir = \"/scratch1/ssawicki/cache\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"<|ELIF|> Question: %question\\nAnswer: \"\n",
    "\n",
    "def generate_answers(ds_item, model, tokenizer, model_name):\n",
    "    # quantization config\n",
    "\n",
    "    generation_config = GenerationConfig(\n",
    "        top_k=0.0,\n",
    "        top_p=1.0,\n",
    "        do_sample=True,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        max_length=1024\n",
    "    )\n",
    "    \n",
    "    pipeline = TextGenerationPipeline(model=model, tokenizer=tokenizer)\n",
    "\n",
    "    answers = {}\n",
    "\n",
    "    prompt = template.replace('%question', ds_item['question_title'])\n",
    "    result = pipeline(prompt, generation_config=generation_config, return_full_text=False)\n",
    "    generated_text = result[0]['generated_text']\n",
    "    ds_item[model_name] = generated_text\n",
    "    return ds_item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 8/8 [00:05<00:00,  1.54it/s]\n",
      "Map: 100%|██████████| 100/100 [40:32<00:00, 24.32s/ examples]\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:03<00:00,  1.21s/it]\n",
      "Map: 100%|██████████| 100/100 [25:06<00:00, 15.06s/ examples]\n"
     ]
    }
   ],
   "source": [
    "for model_name, model_path in model_checkpoints.items():\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16\n",
    "    )\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_path,\n",
    "            low_cpu_mem_usage=True,\n",
    "            quantization_config=bnb_config,\n",
    "            cache_dir=cache_dir\n",
    "        )\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path, cache_dir=cache_dir)\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "    model.config.pad_token_id = model.config.eos_token_id\n",
    "    model = model.eval()\n",
    "\n",
    "    answer_generator = partial(generate_answers, model=model, tokenizer=tokenizer, model_name=model_name)\n",
    "    dataset = dataset.map(answer_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 100/100 [00:00<00:00, 20975.72 examples/s]\n"
     ]
    }
   ],
   "source": [
    "dataset.save_to_disk(\"/scratch1/redditqa/cached_datasets/AskHistorians_test_set_100_model_generated\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
