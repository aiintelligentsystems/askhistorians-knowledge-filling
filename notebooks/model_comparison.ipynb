{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jhoffbauer/reddit_qa/venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2023-08-10 12:09:44.785970: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-08-10 12:09:45.894824: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from dataclasses import dataclass, field\n",
    "from typing import Optional\n",
    "import huggingface_hub\n",
    "import functools as ft\n",
    "import torch\n",
    "import pandas as pd\n",
    "import torch\n",
    "from accelerate import Accelerator\n",
    "from datasets import load_dataset\n",
    "from peft import LoraConfig\n",
    "from tqdm import tqdm\n",
    "from transformers import (\n",
    "    Adafactor,\n",
    "    AutoTokenizer,\n",
    "    HfArgumentParser,\n",
    "    pipeline,\n",
    "    AutoConfig,\n",
    "    GPTNeoXForCausalLM,\n",
    "    AutoModelForCausalLM,\n",
    ")\n",
    "from trl import AutoModelForCausalLMWithValueHead, PPOConfig, PPOTrainer, set_seed\n",
    "from trl.core import LengthSampler\n",
    "from transformers import pipeline, TextGenerationPipeline\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from redditqa.dataset import load_reddit_dataset\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Optional\n",
    "import huggingface_hub\n",
    "import functools as ft\n",
    "import torch\n",
    "import pandas as pd\n",
    "import torch\n",
    "import os\n",
    "from accelerate import Accelerator\n",
    "from datasets import load_dataset\n",
    "from peft import LoraConfig\n",
    "from tqdm import tqdm\n",
    "from transformers import (\n",
    "    Adafactor,\n",
    "    AutoTokenizer,\n",
    "    HfArgumentParser,\n",
    "    pipeline,\n",
    "    AutoConfig,\n",
    "    GPTNeoXForCausalLM,\n",
    "    AutoModelForCausalLM,\n",
    ")\n",
    "from trl import AutoModelForCausalLMWithValueHead, PPOConfig, PPOTrainer, set_seed\n",
    "from trl.core import LengthSampler\n",
    "from transformers import (\n",
    "    pipeline,\n",
    "    TextGenerationPipeline,\n",
    "    AutoConfig,\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    AutoModel,\n",
    "    AutoModelForSequenceClassification,\n",
    "    GPTNeoXForCausalLM,\n",
    "    LlamaForSequenceClassification,\n",
    ")\n",
    "from redditqa.dataset import load_reddit_dataset\n",
    "from transformers.utils.hub import convert_file_size_to_int, get_checkpoint_shard_files\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    PeftConfig,\n",
    "    PeftModel,\n",
    "    TaskType,\n",
    "    get_peft_model,\n",
    "    set_peft_model_state_dict,\n",
    ")\n",
    "from peft.utils import _get_submodules\n",
    "import peft\n",
    "import torch\n",
    "import pandas as pd\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from statsmodels.stats import inter_rater as irr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading the eval dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /scratch1/jhoff/reddit_dataset_cached/eval/cache-cba55e4212677d14.arrow\n",
      "Loading cached shuffled indices for dataset at /scratch1/jhoff/reddit_dataset_cached/eval/cache-71c7c533e04253a7.arrow\n"
     ]
    }
   ],
   "source": [
    "eval_dataset = load_reddit_dataset(\"eval\")\n",
    "eval_dataset = eval_dataset.select(range(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = eval_dataset['question_title']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoints = {\n",
    "    'baseline': 'meta-llama/Llama-2-7b-chat-hf',\n",
    "    'checkpoint-2500': '/scratch1/jhoff/checkpoints/finetuned_llama-2-7b-chat-hf/checkpoint-2500_merged',\n",
    "    'checkpoint-8500': '/scratch1/jhoff/checkpoints/finetuned_llama-2-7b-chat-hf/checkpoint-8500_merged',\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the models and make sure they actually are different"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading baseline from meta-llama/Llama-2-7b-chat-hf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "models = {}\n",
    "for name, checkpoint_path in model_checkpoints.items():\n",
    "    print(f\"Loading {name} from {checkpoint_path}\")\n",
    "    model = AutoModelForCausalLM.from_pretrained(checkpoint_path, torch_dtype=torch.bfloat16)\n",
    "    model.config.pad_token_id = model.config.eos_token_id\n",
    "    models[name] = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.0060, -0.0146, -0.0021,  ...,  0.0042,  0.0018, -0.0035],\n",
       "        [ 0.0142, -0.0043,  0.0032,  ..., -0.0092, -0.0108,  0.0073],\n",
       "        [-0.0137,  0.0121,  0.0002,  ...,  0.0061,  0.0181, -0.0030],\n",
       "        ...,\n",
       "        [ 0.0018,  0.0093, -0.0006,  ...,  0.0092, -0.0289,  0.0085],\n",
       "        [ 0.0249,  0.0116,  0.0035,  ..., -0.0322, -0.0165, -0.0111],\n",
       "        [-0.0136, -0.0067,  0.0016,  ...,  0.0176,  0.0175, -0.0083]],\n",
       "       dtype=torch.bfloat16, requires_grad=True)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models['baseline'].model.layers[0].self_attn.q_proj.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.0057, -0.0152, -0.0024,  ...,  0.0041,  0.0019, -0.0035],\n",
       "        [ 0.0140, -0.0042,  0.0029,  ..., -0.0090, -0.0108,  0.0072],\n",
       "        [-0.0139,  0.0126,  0.0006,  ...,  0.0059,  0.0182, -0.0029],\n",
       "        ...,\n",
       "        [ 0.0016,  0.0093, -0.0006,  ...,  0.0087, -0.0284,  0.0085],\n",
       "        [ 0.0250,  0.0115,  0.0036,  ..., -0.0322, -0.0165, -0.0109],\n",
       "        [-0.0134, -0.0066,  0.0022,  ...,  0.0179,  0.0170, -0.0082]],\n",
       "       dtype=torch.bfloat16, requires_grad=True)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models['checkpoint-2500'].model.layers[0].self_attn.q_proj.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-5.6152e-03, -1.5503e-02, -2.9297e-03,  ...,  3.3875e-03,\n",
       "          2.0294e-03, -3.2196e-03],\n",
       "        [ 1.3977e-02, -4.3335e-03,  2.9144e-03,  ..., -8.3618e-03,\n",
       "         -1.1108e-02,  7.1106e-03],\n",
       "        [-1.3855e-02,  1.3062e-02,  1.3199e-03,  ...,  6.7749e-03,\n",
       "          1.7944e-02, -3.1586e-03],\n",
       "        ...,\n",
       "        [ 1.5564e-03,  1.0254e-02,  4.6253e-05,  ...,  7.4158e-03,\n",
       "         -2.8198e-02,  8.9722e-03],\n",
       "        [ 2.5513e-02,  1.1597e-02,  3.9368e-03,  ..., -3.1250e-02,\n",
       "         -1.6724e-02, -1.1108e-02],\n",
       "        [-1.2939e-02, -8.3008e-03,  1.1063e-03,  ...,  1.9897e-02,\n",
       "          1.6479e-02, -8.9111e-03]], dtype=torch.bfloat16, requires_grad=True)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models['checkpoint-8500'].model.layers[0].self_attn.q_proj.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading shards: 100%|██████████| 2/2 [00:00<00:00,  7.79it/s]\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [02:40<00:00, 80.27s/it] \n",
      "Downloading (…)neration_config.json: 100%|██████████| 188/188 [00:00<00:00, 2.38MB/s]\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "/scratch1/jhoff/checkpoints/finetuned_llama-2-7b-chat-hf/checkpoint-2500 does not appear to have a file named config.json. Checkout 'https://huggingface.co//scratch1/jhoff/checkpoints/finetuned_llama-2-7b-chat-hf/checkpoint-2500/None' for available files.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 35\u001b[0m\n\u001b[1;32m     33\u001b[0m model_answers \u001b[39m=\u001b[39m {}\n\u001b[1;32m     34\u001b[0m \u001b[39mfor\u001b[39;00m model_name, checkpoint \u001b[39min\u001b[39;00m model_checkpoints\u001b[39m.\u001b[39mitems():\n\u001b[0;32m---> 35\u001b[0m     model_answers[model_name] \u001b[39m=\u001b[39m generate_answers(checkpoint)\n",
      "Cell \u001b[0;32mIn[14], line 5\u001b[0m, in \u001b[0;36mgenerate_answers\u001b[0;34m(model_path)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mgenerate_answers\u001b[39m(model_path):\n\u001b[0;32m----> 5\u001b[0m     model \u001b[39m=\u001b[39m AutoModelForCausalLM\u001b[39m.\u001b[39;49mfrom_pretrained(model_path, torch_dtype\u001b[39m=\u001b[39;49mtorch\u001b[39m.\u001b[39;49mbfloat16)\n\u001b[1;32m      6\u001b[0m     tokenizer \u001b[39m=\u001b[39m AutoTokenizer\u001b[39m.\u001b[39mfrom_pretrained(model_path)\n\u001b[1;32m      7\u001b[0m     model\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mpad_token_id \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39meos_token_id\n",
      "File \u001b[0;32m~/reddit_qa/venv/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:479\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    476\u001b[0m \u001b[39mif\u001b[39;00m kwargs\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mtorch_dtype\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m) \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mauto\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    477\u001b[0m     _ \u001b[39m=\u001b[39m kwargs\u001b[39m.\u001b[39mpop(\u001b[39m\"\u001b[39m\u001b[39mtorch_dtype\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 479\u001b[0m config, kwargs \u001b[39m=\u001b[39m AutoConfig\u001b[39m.\u001b[39;49mfrom_pretrained(\n\u001b[1;32m    480\u001b[0m     pretrained_model_name_or_path,\n\u001b[1;32m    481\u001b[0m     return_unused_kwargs\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    482\u001b[0m     trust_remote_code\u001b[39m=\u001b[39;49mtrust_remote_code,\n\u001b[1;32m    483\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mhub_kwargs,\n\u001b[1;32m    484\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m    485\u001b[0m )\n\u001b[1;32m    487\u001b[0m \u001b[39m# if torch_dtype=auto was passed here, ensure to pass it on\u001b[39;00m\n\u001b[1;32m    488\u001b[0m \u001b[39mif\u001b[39;00m kwargs_orig\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mtorch_dtype\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m) \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mauto\u001b[39m\u001b[39m\"\u001b[39m:\n",
      "File \u001b[0;32m~/reddit_qa/venv/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py:1004\u001b[0m, in \u001b[0;36mAutoConfig.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m   1002\u001b[0m kwargs[\u001b[39m\"\u001b[39m\u001b[39mname_or_path\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m pretrained_model_name_or_path\n\u001b[1;32m   1003\u001b[0m trust_remote_code \u001b[39m=\u001b[39m kwargs\u001b[39m.\u001b[39mpop(\u001b[39m\"\u001b[39m\u001b[39mtrust_remote_code\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m)\n\u001b[0;32m-> 1004\u001b[0m config_dict, unused_kwargs \u001b[39m=\u001b[39m PretrainedConfig\u001b[39m.\u001b[39;49mget_config_dict(pretrained_model_name_or_path, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1005\u001b[0m has_remote_code \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mauto_map\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m config_dict \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mAutoConfig\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m config_dict[\u001b[39m\"\u001b[39m\u001b[39mauto_map\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m   1006\u001b[0m has_local_code \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mmodel_type\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m config_dict \u001b[39mand\u001b[39;00m config_dict[\u001b[39m\"\u001b[39m\u001b[39mmodel_type\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39min\u001b[39;00m CONFIG_MAPPING\n",
      "File \u001b[0;32m~/reddit_qa/venv/lib/python3.10/site-packages/transformers/configuration_utils.py:620\u001b[0m, in \u001b[0;36mPretrainedConfig.get_config_dict\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    618\u001b[0m original_kwargs \u001b[39m=\u001b[39m copy\u001b[39m.\u001b[39mdeepcopy(kwargs)\n\u001b[1;32m    619\u001b[0m \u001b[39m# Get config dict associated with the base config file\u001b[39;00m\n\u001b[0;32m--> 620\u001b[0m config_dict, kwargs \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39;49m\u001b[39m.\u001b[39;49m_get_config_dict(pretrained_model_name_or_path, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    621\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39m_commit_hash\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m config_dict:\n\u001b[1;32m    622\u001b[0m     original_kwargs[\u001b[39m\"\u001b[39m\u001b[39m_commit_hash\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m config_dict[\u001b[39m\"\u001b[39m\u001b[39m_commit_hash\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[0;32m~/reddit_qa/venv/lib/python3.10/site-packages/transformers/configuration_utils.py:675\u001b[0m, in \u001b[0;36mPretrainedConfig._get_config_dict\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    671\u001b[0m configuration_file \u001b[39m=\u001b[39m kwargs\u001b[39m.\u001b[39mpop(\u001b[39m\"\u001b[39m\u001b[39m_configuration_file\u001b[39m\u001b[39m\"\u001b[39m, CONFIG_NAME)\n\u001b[1;32m    673\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    674\u001b[0m     \u001b[39m# Load from local folder or from cache or download from model Hub and cache\u001b[39;00m\n\u001b[0;32m--> 675\u001b[0m     resolved_config_file \u001b[39m=\u001b[39m cached_file(\n\u001b[1;32m    676\u001b[0m         pretrained_model_name_or_path,\n\u001b[1;32m    677\u001b[0m         configuration_file,\n\u001b[1;32m    678\u001b[0m         cache_dir\u001b[39m=\u001b[39;49mcache_dir,\n\u001b[1;32m    679\u001b[0m         force_download\u001b[39m=\u001b[39;49mforce_download,\n\u001b[1;32m    680\u001b[0m         proxies\u001b[39m=\u001b[39;49mproxies,\n\u001b[1;32m    681\u001b[0m         resume_download\u001b[39m=\u001b[39;49mresume_download,\n\u001b[1;32m    682\u001b[0m         local_files_only\u001b[39m=\u001b[39;49mlocal_files_only,\n\u001b[1;32m    683\u001b[0m         token\u001b[39m=\u001b[39;49mtoken,\n\u001b[1;32m    684\u001b[0m         user_agent\u001b[39m=\u001b[39;49muser_agent,\n\u001b[1;32m    685\u001b[0m         revision\u001b[39m=\u001b[39;49mrevision,\n\u001b[1;32m    686\u001b[0m         subfolder\u001b[39m=\u001b[39;49msubfolder,\n\u001b[1;32m    687\u001b[0m         _commit_hash\u001b[39m=\u001b[39;49mcommit_hash,\n\u001b[1;32m    688\u001b[0m     )\n\u001b[1;32m    689\u001b[0m     commit_hash \u001b[39m=\u001b[39m extract_commit_hash(resolved_config_file, commit_hash)\n\u001b[1;32m    690\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mEnvironmentError\u001b[39;00m:\n\u001b[1;32m    691\u001b[0m     \u001b[39m# Raise any environment error raise by `cached_file`. It will have a helpful error message adapted to\u001b[39;00m\n\u001b[1;32m    692\u001b[0m     \u001b[39m# the original exception.\u001b[39;00m\n",
      "File \u001b[0;32m~/reddit_qa/venv/lib/python3.10/site-packages/transformers/utils/hub.py:398\u001b[0m, in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    396\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39misfile(resolved_file):\n\u001b[1;32m    397\u001b[0m     \u001b[39mif\u001b[39;00m _raise_exceptions_for_missing_entries:\n\u001b[0;32m--> 398\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mEnvironmentError\u001b[39;00m(\n\u001b[1;32m    399\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mpath_or_repo_id\u001b[39m}\u001b[39;00m\u001b[39m does not appear to have a file named \u001b[39m\u001b[39m{\u001b[39;00mfull_filename\u001b[39m}\u001b[39;00m\u001b[39m. Checkout \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    400\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39mhttps://huggingface.co/\u001b[39m\u001b[39m{\u001b[39;00mpath_or_repo_id\u001b[39m}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{\u001b[39;00mrevision\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m for available files.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    401\u001b[0m         )\n\u001b[1;32m    402\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    403\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "\u001b[0;31mOSError\u001b[0m: /scratch1/jhoff/checkpoints/finetuned_llama-2-7b-chat-hf/checkpoint-2500 does not appear to have a file named config.json. Checkout 'https://huggingface.co//scratch1/jhoff/checkpoints/finetuned_llama-2-7b-chat-hf/checkpoint-2500/None' for available files."
     ]
    }
   ],
   "source": [
    "template = \"<|ELIF|> Question: %question\\nAnswer: \"\n",
    "\n",
    "def generate_answers(model_path):\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_path, torch_dtype=torch.bfloat16)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "    model.config.pad_token_id = model.config.eos_token_id\n",
    "    model = model.cuda().eval()\n",
    "\n",
    "    generation_kwargs = {\n",
    "        \"top_k\": 0.0,\n",
    "        \"top_p\": 1.0,\n",
    "        \"do_sample\": True,\n",
    "        \"pad_token_id\": tokenizer.pad_token_id,\n",
    "        \"min_length\": 32,\n",
    "        \"max_length\": 128,\n",
    "    }\n",
    "\n",
    "    pipeline = TextGenerationPipeline(model=model, tokenizer=tokenizer, device='cuda:0')\n",
    "\n",
    "    answers = {}\n",
    "\n",
    "    for question in questions:\n",
    "        prompt = template.replace('%question', question)\n",
    "        result = pipeline(prompt, **generation_kwargs, return_full_text=False)\n",
    "        result = result[0]['generated_text']\n",
    "\n",
    "        answers[question] = result\n",
    "\n",
    "    return answers\n",
    "\n",
    "\n",
    "model_answers = {}\n",
    "for model_name, checkpoint in model_checkpoints.items():\n",
    "    model_answers[model_name] = generate_answers(checkpoint)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
