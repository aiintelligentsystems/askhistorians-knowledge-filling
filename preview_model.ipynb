{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass, field\n",
    "from typing import Optional\n",
    "import huggingface_hub\n",
    "\n",
    "import torch\n",
    "from accelerate import Accelerator\n",
    "from datasets import load_dataset\n",
    "from peft import LoraConfig\n",
    "from tqdm import tqdm\n",
    "from transformers import Adafactor, AutoTokenizer, HfArgumentParser, pipeline, AutoConfig, GPTNeoXForCausalLM\n",
    "from trl import AutoModelForCausalLMWithValueHead, PPOConfig, PPOTrainer, set_seed\n",
    "from trl.core import LengthSampler\n",
    "from transformers import pipeline, TextGenerationPipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTNeoXForCausalLM(\n",
       "  (gpt_neox): GPTNeoXModel(\n",
       "    (embed_in): Embedding(50304, 768)\n",
       "    (layers): ModuleList(\n",
       "      (0-11): 12 x GPTNeoXLayer(\n",
       "        (input_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (post_attention_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attention): GPTNeoXAttention(\n",
       "          (rotary_emb): RotaryEmbedding()\n",
       "          (query_key_value): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (mlp): GPTNeoXMLP(\n",
       "          (dense_h_to_4h): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (dense_4h_to_h): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (act): GELUActivation()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (embed_out): Linear(in_features=768, out_features=50304, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = GPTNeoXForCausalLM.from_pretrained(\"/scratch1/jhoff/checkpoints/generator/runs/final_merged\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"/scratch1/jhoff/checkpoints/generator/runs/final_merged\")\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<transformers.pipelines.text_generation.TextGenerationPipeline at 0x7f9e9fddfc40>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline = TextGenerationPipeline(model=model, tokenizer=tokenizer)\n",
    "pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "/home/jhoffbauer/reddit_qa/venv/lib/python3.10/site-packages/transformers/generation/utils.py:1349: UserWarning: Using `max_length`'s default (20) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--\n",
      "Question: What is 1+1?\n",
      "    Answer: 1+1\n",
      "\n",
      "A:\n",
      "\n",
      "--\n",
      "Output length: 43\n",
      "Output tokens: 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--\n",
      "Question: What is 2+2?\n",
      "    Answer: 2+2\n",
      "\n",
      "A:\n",
      "\n",
      "--\n",
      "Output length: 43\n",
      "Output tokens: 20\n",
      "--\n",
      "Question: What is 3+3?\n",
      "    Answer: 3+3\n",
      "\n",
      "A:\n",
      "\n",
      "--\n",
      "Output length: 43\n",
      "Output tokens: 20\n"
     ]
    }
   ],
   "source": [
    "for question in [\"What is 1+1?\", \"What is 2+2?\", \"What is 3+3?\"]:\n",
    "    result = pipeline(f\"\"\"Question: {question}\n",
    "    Answer:\"\"\")\n",
    "    result = result[0]['generated_text']\n",
    "    print('-' * 8)\n",
    "    print(f'{result}')\n",
    "    print('-')\n",
    "    print(f\"Output length: {len(result)}\")\n",
    "    print(f\"Output tokens: {tokenizer(result)['input_ids'].__len__()}\")\n",
    "    print('-' * 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Question: What is 1+1?\\nAnswer: 1+1\\n\\nA:\\n\\n'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTNeoXConfig {\n",
       "  \"_name_or_path\": \"/scratch1/jhoff/checkpoints/generator/runs/final_merged\",\n",
       "  \"architectures\": [\n",
       "    \"GPTNeoXForCausalLM\"\n",
       "  ],\n",
       "  \"bos_token_id\": 0,\n",
       "  \"classifier_dropout\": 0.1,\n",
       "  \"eos_token_id\": 0,\n",
       "  \"hidden_act\": \"gelu\",\n",
       "  \"hidden_size\": 768,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 3072,\n",
       "  \"layer_norm_eps\": 1e-05,\n",
       "  \"max_position_embeddings\": 2048,\n",
       "  \"model_type\": \"gpt_neox\",\n",
       "  \"num_attention_heads\": 12,\n",
       "  \"num_hidden_layers\": 12,\n",
       "  \"rotary_emb_base\": 10000,\n",
       "  \"rotary_pct\": 0.25,\n",
       "  \"tie_word_embeddings\": false,\n",
       "  \"torch_dtype\": \"float32\",\n",
       "  \"transformers_version\": \"4.30.0.dev0\",\n",
       "  \"use_cache\": true,\n",
       "  \"use_parallel_residual\": true,\n",
       "  \"vocab_size\": 50304\n",
       "}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
