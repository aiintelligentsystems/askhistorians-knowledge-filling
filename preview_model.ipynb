{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jhoffbauer/reddit_qa/venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please run\n",
      "\n",
      "python -m bitsandbytes\n",
      "\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "/home/jhoffbauer/reddit_qa/venv/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cpu.so: undefined symbol: cadam32bit_grad_fp32\n",
      "CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching in backup paths...\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 8.6\n",
      "CUDA SETUP: Detected CUDA version 117\n",
      "CUDA SETUP: Loading binary /home/jhoffbauer/reddit_qa/venv/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cpu.so...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jhoffbauer/reddit_qa/venv/lib/python3.10/site-packages/bitsandbytes/cextension.py:34: UserWarning: The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n",
      "  warn(\"The installed version of bitsandbytes was compiled without GPU support. \"\n",
      "/home/jhoffbauer/reddit_qa/venv/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:147: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('vs/workbench/api/node/extensionHostProcess')}\n",
      "  warn(msg)\n",
      "/home/jhoffbauer/reddit_qa/venv/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:147: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('module'), PosixPath('//matplotlib_inline.backend_inline')}\n",
      "  warn(msg)\n",
      "/home/jhoffbauer/reddit_qa/venv/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:147: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/usr/local/cuda/lib64')}\n",
      "  warn(msg)\n",
      "/home/jhoffbauer/reddit_qa/venv/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:147: UserWarning: WARNING: No libcudart.so found! Install CUDA or the cudatoolkit package (anaconda)!\n",
      "  warn(msg)\n"
     ]
    }
   ],
   "source": [
    "from dataclasses import dataclass, field\n",
    "from typing import Optional\n",
    "import huggingface_hub\n",
    "\n",
    "import torch\n",
    "from accelerate import Accelerator\n",
    "from datasets import load_dataset\n",
    "from peft import LoraConfig\n",
    "from tqdm import tqdm\n",
    "from transformers import Adafactor, AutoTokenizer, HfArgumentParser, pipeline, AutoConfig, GPTNeoXForCausalLM\n",
    "from trl import AutoModelForCausalLMWithValueHead, PPOConfig, PPOTrainer, set_seed\n",
    "from trl.core import LengthSampler\n",
    "from transformers import pipeline, TextGenerationPipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[01;34mgenerator_pythia-160M\u001b[0m/\n",
      "\u001b[01;34mgenerator_pythia-1B\u001b[0m/\n",
      "\u001b[01;34mgenerator_pythia-1B-Dedupedstep_0\u001b[0m/\n",
      "\u001b[01;34mgenerator_pythia-2.8B-Deduped_fix-sentimentstep_0\u001b[0m/\n",
      "\u001b[01;34mgenerator_pythia-2.8B-Deduped_fix-sentimentstep_1000\u001b[0m/\n",
      "\u001b[01;34mgenerator_pythia-2.8B-Deduped_fix-sentimentstep_10000\u001b[0m/\n",
      "\u001b[01;34mgenerator_pythia-2.8B-Deduped_fix-sentimentstep_11000\u001b[0m/\n",
      "\u001b[01;34mgenerator_pythia-2.8B-Deduped_fix-sentimentstep_12000\u001b[0m/\n",
      "\u001b[01;34mgenerator_pythia-2.8B-Deduped_fix-sentimentstep_13000\u001b[0m/\n",
      "\u001b[01;34mgenerator_pythia-2.8B-Deduped_fix-sentimentstep_2000\u001b[0m/\n",
      "\u001b[01;34mgenerator_pythia-2.8B-Deduped_fix-sentimentstep_3000\u001b[0m/\n",
      "\u001b[01;34mgenerator_pythia-2.8B-Deduped_fix-sentimentstep_4000\u001b[0m/\n",
      "\u001b[01;34mgenerator_pythia-2.8B-Deduped_fix-sentimentstep_5000\u001b[0m/\n",
      "\u001b[01;34mgenerator_pythia-2.8B-Deduped_fix-sentimentstep_6000\u001b[0m/\n",
      "\u001b[01;34mgenerator_pythia-2.8B-Deduped_fix-sentimentstep_7000\u001b[0m/\n",
      "\u001b[01;34mgenerator_pythia-2.8B-Deduped_fix-sentimentstep_8000\u001b[0m/\n",
      "\u001b[01;34mgenerator_pythia-2.8B-Deduped_fix-sentimentstep_9000\u001b[0m/\n",
      "\u001b[01;34mgenerator_pythia-2.8B-Dedupedstep_0\u001b[0m/\n",
      "\u001b[01;34mgenerator_pythia-2.8B-Dedupedstep_1000\u001b[0m/\n",
      "\u001b[01;34mgenerator_pythia-2.8B-Dedupedstep_2000\u001b[0m/\n",
      "\u001b[01;34mreward-model_pythia-160M\u001b[0m/\n",
      "\u001b[01;34mreward_pythia-1B-Deduped_peft\u001b[0m/\n",
      "\u001b[01;34mreward_pythia-2.8B-Deduped_peft\u001b[0m/\n",
      "\u001b[01;34mtmp_trainer\u001b[0m/\n"
     ]
    }
   ],
   "source": [
    "%ls /scratch1/jhoff/checkpoints/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading (…)okenizer_config.json: 100%|██████████| 396/396 [00:00<00:00, 1.45MB/s]\n",
      "Downloading (…)/main/tokenizer.json: 2.11MB [00:00, 5.72MB/s]\n",
      "Downloading (…)cial_tokens_map.json: 100%|██████████| 99.0/99.0 [00:00<00:00, 747kB/s]\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 64.00 MiB (GPU 0; 23.69 GiB total capacity; 12.71 GiB already allocated; 60.06 MiB free; 12.73 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m tokenizer \u001b[39m=\u001b[39m AutoTokenizer\u001b[39m.\u001b[39mfrom_pretrained(\u001b[39m\"\u001b[39m\u001b[39mEleutherAI/Pythia-1B-Deduped\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      5\u001b[0m model\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mpad_token_id \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39meos_token_id\n\u001b[0;32m----> 6\u001b[0m model \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mcuda()\u001b[39m.\u001b[39meval()\n",
      "File \u001b[0;32m~/reddit_qa/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:905\u001b[0m, in \u001b[0;36mModule.cuda\u001b[0;34m(self, device)\u001b[0m\n\u001b[1;32m    888\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcuda\u001b[39m(\u001b[39mself\u001b[39m: T, device: Optional[Union[\u001b[39mint\u001b[39m, device]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m T:\n\u001b[1;32m    889\u001b[0m \u001b[39m    \u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"Moves all model parameters and buffers to the GPU.\u001b[39;00m\n\u001b[1;32m    890\u001b[0m \n\u001b[1;32m    891\u001b[0m \u001b[39m    This also makes associated parameters and buffers different objects. So\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    903\u001b[0m \u001b[39m        Module: self\u001b[39;00m\n\u001b[1;32m    904\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 905\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_apply(\u001b[39mlambda\u001b[39;49;00m t: t\u001b[39m.\u001b[39;49mcuda(device))\n",
      "File \u001b[0;32m~/reddit_qa/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:797\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    795\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_apply\u001b[39m(\u001b[39mself\u001b[39m, fn):\n\u001b[1;32m    796\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchildren():\n\u001b[0;32m--> 797\u001b[0m         module\u001b[39m.\u001b[39;49m_apply(fn)\n\u001b[1;32m    799\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    800\u001b[0m         \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    801\u001b[0m             \u001b[39m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    802\u001b[0m             \u001b[39m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    807\u001b[0m             \u001b[39m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    808\u001b[0m             \u001b[39m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/reddit_qa/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:797\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    795\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_apply\u001b[39m(\u001b[39mself\u001b[39m, fn):\n\u001b[1;32m    796\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchildren():\n\u001b[0;32m--> 797\u001b[0m         module\u001b[39m.\u001b[39;49m_apply(fn)\n\u001b[1;32m    799\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    800\u001b[0m         \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    801\u001b[0m             \u001b[39m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    802\u001b[0m             \u001b[39m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    807\u001b[0m             \u001b[39m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    808\u001b[0m             \u001b[39m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "    \u001b[0;31m[... skipping similar frames: Module._apply at line 797 (2 times)]\u001b[0m\n",
      "File \u001b[0;32m~/reddit_qa/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:797\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    795\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_apply\u001b[39m(\u001b[39mself\u001b[39m, fn):\n\u001b[1;32m    796\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchildren():\n\u001b[0;32m--> 797\u001b[0m         module\u001b[39m.\u001b[39;49m_apply(fn)\n\u001b[1;32m    799\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    800\u001b[0m         \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    801\u001b[0m             \u001b[39m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    802\u001b[0m             \u001b[39m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    807\u001b[0m             \u001b[39m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    808\u001b[0m             \u001b[39m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/reddit_qa/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:820\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    816\u001b[0m \u001b[39m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    817\u001b[0m \u001b[39m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    818\u001b[0m \u001b[39m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    819\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m--> 820\u001b[0m     param_applied \u001b[39m=\u001b[39m fn(param)\n\u001b[1;32m    821\u001b[0m should_use_set_data \u001b[39m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    822\u001b[0m \u001b[39mif\u001b[39;00m should_use_set_data:\n",
      "File \u001b[0;32m~/reddit_qa/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:905\u001b[0m, in \u001b[0;36mModule.cuda.<locals>.<lambda>\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    888\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcuda\u001b[39m(\u001b[39mself\u001b[39m: T, device: Optional[Union[\u001b[39mint\u001b[39m, device]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m T:\n\u001b[1;32m    889\u001b[0m \u001b[39m    \u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"Moves all model parameters and buffers to the GPU.\u001b[39;00m\n\u001b[1;32m    890\u001b[0m \n\u001b[1;32m    891\u001b[0m \u001b[39m    This also makes associated parameters and buffers different objects. So\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    903\u001b[0m \u001b[39m        Module: self\u001b[39;00m\n\u001b[1;32m    904\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 905\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_apply(\u001b[39mlambda\u001b[39;00m t: t\u001b[39m.\u001b[39;49mcuda(device))\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 64.00 MiB (GPU 0; 23.69 GiB total capacity; 12.71 GiB already allocated; 60.06 MiB free; 12.73 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "#model_path = '/scratch1/jhoff/checkpoints/generator_pythia-1B/runs/step_8000_merged'\n",
    "model_path = \"EleutherAI/pythia-1B-Deduped\"\n",
    "model = GPTNeoXForCausalLM.from_pretrained(model_path)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/Pythia-1B-Deduped\")\n",
    "model.config.pad_token_id = model.config.eos_token_id\n",
    "model = model.cuda().eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<transformers.pipelines.text_generation.TextGenerationPipeline at 0x7f7d8b6346d0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline = TextGenerationPipeline(model=model, tokenizer=tokenizer, device=0)\n",
    "pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jhoffbauer/reddit_qa/venv/lib/python3.10/site-packages/transformers/generation/utils.py:1255: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation)\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "Question: ELI5: What is 1+1?\n",
      "\n",
      "\n",
      "Answer: 2\n",
      "\n",
      "\n",
      "\n",
      "Question: ELI5: Given an equation relating two integers n and k. What is the n=k+1 solution?\n",
      "\n",
      "\n",
      "Answer: if you draw a rook on the given diagram, the diagonal has the types NA, AB.\n",
      "\n",
      "\n",
      "Question: ELI5: How do you know that two unchecked field offers are not coincident?\n",
      "\n",
      "\n",
      "Answer: notice that empty squares are excluded from both lists.\n",
      "\n",
      "\n",
      "\n",
      "Question: ELI5: What is the length of the diagonal\n",
      "\n",
      "of the Young diagram of size\n",
      "-\n",
      "Output length: 458\n",
      "Output tokens: 128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "Question: ELI5: The 3:1 microphone rule\n",
      "\n",
      "\n",
      "Answer: There is a speaker-position panel where you can turn the sound up and up. This positioning panel is right beneath the grill of the rear box in the downbox. Take a look at figure 4 of section 8.1.15.\n",
      "\n",
      "\n",
      "\n",
      "Request: ELI5: Where in the CTS-V is the removable console?\n",
      "\n",
      "NOTE: The \"console\" is a removable tray for the game/music/CD drives (see teaser image for next section). Nuttin' under the hood, honey.\n",
      "\n",
      "\n",
      "\n",
      "QUESTION:\n",
      "-\n",
      "Output length: 462\n",
      "Output tokens: 128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "Question: ELI5: why are humans considered three dimensional beings when we also move through the fourth dimension of time?\n",
      "for example, in interstellar and other sci-fi or theoretical scenarious we hear about 'four-dimensional beings'. But are humans not already 'four-dimensional beings' if we move through the fourth dimension that is time? \n",
      "\n",
      "\n",
      "Answer: The fourth dimension is not the space between all other three dimensions; as one of our wanderers has said, it is the dimension space is emptied of. Translating that to human expression: it is the dimension of in-betweenness, identity, and relation\n",
      "-\n",
      "Output length: 602\n",
      "Output tokens: 128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "Question: ELI5: Would there be any legal ramifications if a major company were to use an idea from a website like Reddit?\n",
      "For example, Universal Studio uses an idea from r/writingprompt to create a film. If not, should we not create a system to protect out ideas from being stolen? \n",
      "\n",
      "\n",
      "Answer: SJF\n",
      "\n",
      "Unfortunately, with the current state of copyright law, you cannot do this legally.\n",
      "\n",
      "The basic law is clear: \"Ownership belongs to the author or creator of a work. There is no right or license that gives one person the right to use, modify, publish\n",
      "-\n",
      "Output length: 546\n",
      "Output tokens: 128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "Question: ELI5: What's the difference between blu-ray quality and 1080p? \n",
      "Also, what is a common limiting factor of quality when using a tv or monitor? \n",
      "\n",
      "\n",
      "Answer: Both formats are different and can look completely different. Also very depending on quality of the source format (blu-ray, dvd..., hevc, etc)\n",
      "\n",
      "https://sm.nge-is.com/profiles/10/n_525.html#s_10\n",
      "https://sm.nge-is.com/about-us/video-editing/digital-animation/\n",
      "-\n",
      "Output length: 420\n",
      "Output tokens: 128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "Question: ELI5: Why does blood taste like metal? \n",
      "\n",
      "\n",
      "Answer:\n",
      "Credit Where Due:\n",
      "[Orig explanation.   Joe provided 2001's epalyc duel at Tree Fort.\n",
      "See TreeFort section for link.]\n",
      "1.  All blood has bentonite suspended in it.  (The acids in blood form quick salt that clings to particles via H-bonding, thus causing blood to be like metal!  Note: This explanation was not due to Joe in 2001.  Espitally, the flavor of bentonite, and probably other acidic materials, is what produces that\n",
      "-\n",
      "Output length: 483\n",
      "Output tokens: 128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "Question: ELI5: Why do all of the new hires at my job suck? \n",
      "\n",
      "\n",
      "Answer: ELI5: That is a great question.\n",
      "\n",
      "Unfortunately, I will not write a full answer here, as I have been asked to write one (although I give you all the credit), but I will just answer your sub-question with another sub-question.\n",
      "\n",
      "It’s Enough to Move to the Front of the Store, But it’s Only Slightly Better\n",
      "\n",
      "Back in 2010 and less than 1,000 people read my academic paper on the probability that appointments rise to the top of\n",
      "-\n",
      "Output length: 493\n",
      "Output tokens: 128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "Question: Eli5 Best yo mama jokes please? \n",
      "\n",
      "\n",
      "Answer: \n",
      "\n",
      "\n",
      "   \n",
      "Lunch is the best time to eat- instantly a person has more energy.\n",
      "Pounding the table is a joke to make people feel uncomfortable.\n",
      "Groundhog day is one of the best recenlty.\n",
      "Not all white people are the same. The color of their skin has nothing to do with how they vote.\n",
      "Mouths = mouths.\n",
      "A person may not speak but they can still \"sing\" loud.\n",
      "To sad someone is to write them a letter and before sealing it address it \"sealed\n",
      "-\n",
      "Output length: 484\n",
      "Output tokens: 128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "Question: Eli5: Why can people sue Ashley Madison even though they weren't the ones that released the information? \n",
      "\n",
      "\n",
      "Answer: Because hacker is common law trespasser and a criminal. \n",
      "\n",
      "And nobody is safe, even your mother or your lawyer or the government. Because, by and large, hackers cross all lines (except those where the court says no, like children wandering through a fence), they pay no attention to your property, and their motivations are not the same as yours. \n",
      "\n",
      "Besides, simply saying the information wasn't theirs is not actually true, since they gained it either directly or by violating someone's IP or\n",
      "-\n",
      "Output length: 617\n",
      "Output tokens: 128\n",
      "------------------------------\n",
      "Question: ELI5: Toyota can't make an exact copy of a Porsche 911 bodywork, or a Ferrari, put a Toyota engine and sticker on it and then sell it. However, they can design their cars to be \"sportier\", as a selling point. Where/how is the line drawn between inspiring, and copying? \n",
      "\n",
      "\n",
      "Answer: Good question. Clues to Toyota's cynical approach to building a car *because* it's a car first, rather than as a mechanical challenge, are the striking asymmetrical patterns such as their loving use of stepped vertical slats versus a smoother-looking style\n",
      "-\n",
      "Output length: 546\n",
      "Output tokens: 128\n"
     ]
    }
   ],
   "source": [
    "generation_kwargs = {\n",
    "    \"top_k\": 0.0,\n",
    "    \"top_p\": 1.0,\n",
    "    \"do_sample\": True,\n",
    "    \"pad_token_id\": tokenizer.pad_token_id,\n",
    "    # \"eos_token_id\": 100_000,\n",
    "    \"min_length\": 32,\n",
    "    \"max_length\": 128,\n",
    "}\n",
    "\n",
    "questions = [\n",
    "    \"ELI5: What is 1+1?\",\n",
    "    # \"Who is the president of the U.S.?\",\n",
    "    # \"Why is the sky blue?\", \n",
    "    # \"Please explain climate change proof like I am 5\", \n",
    "    # \"What exactly is Obamacare and what did it change?\", \n",
    "    'ELI5: The 3:1 microphone rule', \n",
    "    \"ELI5: why are humans considered three dimensional beings when we also move through the fourth dimension of time?\\nfor example, in interstellar and other sci-fi or theoretical scenarious we hear about 'four-dimensional beings'. But are humans not already 'four-dimensional beings' if we move through the fourth dimension that is time? \", \n",
    "    'ELI5: Would there be any legal ramifications if a major company were to use an idea from a website like Reddit?\\nFor example, Universal Studio uses an idea from r/writingprompt to create a film. If not, should we not create a system to protect out ideas from being stolen? ', \n",
    "    \"ELI5: What's the difference between blu-ray quality and 1080p? \\nAlso, what is a common limiting factor of quality when using a tv or monitor? \", \n",
    "    'ELI5: Why does blood taste like metal? ', \n",
    "    'ELI5: Why do all of the new hires at my job suck? ', \n",
    "    'Eli5 Best yo mama jokes please? ', \n",
    "    \"Eli5: Why can people sue Ashley Madison even though they weren't the ones that released the information? \", \n",
    "    'ELI5: Toyota can\\'t make an exact copy of a Porsche 911 bodywork, or a Ferrari, put a Toyota engine and sticker on it and then sell it. However, they can design their cars to be \"sportier\", as a selling point. Where/how is the line drawn between inspiring, and copying? ', \n",
    "]      \n",
    "\n",
    "for question in questions:\n",
    "    result = pipeline(f\"\"\"Question: {question}\\n\\n\\nAnswer:\"\"\", **generation_kwargs)\n",
    "    result = result[0]['generated_text']\n",
    "    print('-' * 30)\n",
    "    print(f'{result}')\n",
    "    print('-')\n",
    "    print(f\"Output length: {len(result)}\")\n",
    "    print(f\"Output tokens: {tokenizer(result)['input_ids'].__len__()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTNeoXConfig {\n",
       "  \"_name_or_path\": \"EleutherAI/pythia-2.8B-Deduped\",\n",
       "  \"architectures\": [\n",
       "    \"GPTNeoXForCausalLM\"\n",
       "  ],\n",
       "  \"bos_token_id\": 0,\n",
       "  \"classifier_dropout\": 0.1,\n",
       "  \"eos_token_id\": 0,\n",
       "  \"hidden_act\": \"gelu\",\n",
       "  \"hidden_size\": 2560,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 10240,\n",
       "  \"layer_norm_eps\": 1e-05,\n",
       "  \"max_position_embeddings\": 2048,\n",
       "  \"model_type\": \"gpt_neox\",\n",
       "  \"num_attention_heads\": 32,\n",
       "  \"num_hidden_layers\": 32,\n",
       "  \"pad_token_id\": 0,\n",
       "  \"rotary_emb_base\": 10000,\n",
       "  \"rotary_pct\": 0.25,\n",
       "  \"tie_word_embeddings\": false,\n",
       "  \"torch_dtype\": \"float16\",\n",
       "  \"transformers_version\": \"4.30.0.dev0\",\n",
       "  \"use_cache\": true,\n",
       "  \"use_parallel_residual\": true,\n",
       "  \"vocab_size\": 50304\n",
       "}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jhoffbauer/reddit_qa/venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please run\n",
      "\n",
      "python -m bitsandbytes\n",
      "\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "/home/jhoffbauer/reddit_qa/venv/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cpu.so: undefined symbol: cadam32bit_grad_fp32\n",
      "CUDA SETUP: Loading binary /home/jhoffbauer/reddit_qa/venv/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cpu.so...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jhoffbauer/reddit_qa/venv/lib/python3.10/site-packages/bitsandbytes/cextension.py:34: UserWarning: The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n",
      "  warn(\"The installed version of bitsandbytes was compiled without GPU support. \"\n"
     ]
    }
   ],
   "source": [
    "from dataclasses import dataclass, field\n",
    "from typing import Any, Dict, List, Optional, Union\n",
    "\n",
    "import evaluate\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from datasets import load_dataset\n",
    "from peft import LoraConfig, TaskType, get_peft_model\n",
    "from transformers import (\n",
    "    AutoConfig,\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoTokenizer,\n",
    "    HfArgumentParser,\n",
    "    PreTrainedTokenizerBase,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    ")\n",
    "from transformers.utils import PaddingStrategy\n",
    "\n",
    "from reddit_dataset import load_reddit_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/scratch1/jhoff/checkpoints/reward_pythia-1B-Deduped_peft/checkpoint-250000_adapter\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
